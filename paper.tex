\documentclass[12pt,psamsfonts]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{blkarray}
\usepackage{upquote}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{url}
\usepackage{tikz-cd}
\usepackage{enumerate}
\usepackage{rotating}

\newcommand{\leftshift}{$\,\texttt{<<}\,$}
\newcommand{\downshift}{\mathbin{\rotatebox[origin=c]{90}{\leftshift}}}
\newcommand{\rightshift}{\mathbin{\rotatebox[origin=c]{180}{\leftshift}}}
\newcommand{\upshift}{\mathbin{\rotatebox[origin=c]{270}{\leftshift}}}

\newcommand{\deriv}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\stab}{stab}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Image}{Image}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\real}{Re}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\atan2}{atan2}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\Br}{Br}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\inv}{inv}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\gl}{\mathfrak{gl}}
\DeclareMathOperator{\spl}{\mathfrak{sl}}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\un}{\mathrm{un}}
\newcommand{\al}{\mathrm{al}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\comment}{}

\usepackage[capitalize,nameinlink,noabbrev]{cleveref} % to emulate \autoref style


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
% \newcommand{\theoremautorefname}{Theorem}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\definitionautorefname}{Definition}

\bibliographystyle{plain}
\usepackage{nicematrix}

% \author{Owen Conoly}
% \date{Date}
% \title{Title}

\begin{document}

% \maketitle

\section{Introduction}
We will review the definition of a Springer fiber and define, for a nilpotent \(Y \in \gl_m\), the Springer fiber at the \(n\)-Slodowy slice at \(Y\).
For every \(n\) and every nilpotent \(Y \in \gl_m\), we will find the irreducible components of the Springer fiber at the \(n\)-Slodowy slice at \(Y\).
Finally, we will use our results about Springer fibers at \(n\)-Slodowy slices to find the irreducible components of some other variety (which probably needs a name), and show that they all have the same dimension.

\section{Springer fibers}
Let \(G \subseteq \GL_m(\mathbb{C})\) be a connected semisimple Lie group, and let \(\mathfrak{g} \subseteq \gl_m(\mathbb{C})\) be its Lie algebra.
% ^I take this scope because it is about what Yuzhou Gu said.
Let \(\mathcal{N} \subseteq \mathfrak{g}\) be the subset consisting of nilpotent elements.
Let \(\mathcal{B}\) be the variety of Borel subalgebras of \(\mathfrak{g}\).
Let \(\widetilde{\mathcal{N}} = \{(\mathfrak{b}, n) : n \in \mathfrak{b}\} \subseteq \mathcal{B} \times \mathcal{N}\).
Let \(\pi : \widetilde{\mathcal{N}} \to \mathcal{N}\) be the projection onto the second coordinate.
For \(n \in \mathcal{N}\), we call \(\pi^{-1}(n)\) the \emph{Springer fiber at \(n\)}.
\par We mention some results about Springer fibers, which we will use later in this paper.
\begin{theorem}
    (Needs citation!)
    The irreducible components of the Springer fiber at \(J(\mu)\) are in bijection with the standard Young tableaus of shape \(\mu\).
    Further, the irreducible components are equidimensional, of dimension \(\sum_{i \neq j} \min(\mu_i, \mu_j) = \sum_i (i - 1) \mu_i\).
\end{theorem}

\section{Slodowy slice}
A basis for \(\spl_2(\mathbb{C})\) is
\[e' := \begin{pmatrix}0 & 1 \\ 0 & 0 \\\end{pmatrix}, h' := \begin{pmatrix}1 & 0 \\0 & -1\end{pmatrix}, f' := \begin{pmatrix}0 & 0 \\1 & 0\end{pmatrix}.\]
Given a Lie algebra \(\mathfrak{g}\), and a homomorphism \(\phi : \spl_2 \to \mathfrak{g}\) sending \((e', h', f')\) to \((e, h, f)\), we say that \((e, h, f)\) is an \emph{\(\spl_2\)-triple}.
Observe that \(e, f\) must be nilpotent, and \(h\) must be Cartan (???).
If \(\mathfrak{g}\) is semisimple, then given any nilpotent \(e \in \mathfrak{g}\), the Jacobson-Morozov theorem \cite[3.7.1]{ehf} says that there exist \(h, f \in \mathfrak{g}\) such that \((e, h, f)\) is an \(\spl_2\)-triple.
\par Given \((e, h, f)\), we define the \emph{Slodowy slice at \(e\)} as \(\mathcal{S}_e := e + \ker \ad_f\).
By the Jacobson-Morozov theorem, we can always find a Slodowy slice at any nilpotent \(e \in \mathfrak{g}\), when \(\mathfrak{g}\) is semisimple.

\section{Finding \(\spl_2\)-triples \((E, H, F)\) in \(\gl_{m + n}\) with a particular \(E\).}
\par Let
\[e = \begin{pNiceArray}{ccccc}
 0 \\
    1 & 0 \\
  & 1 & \ddots \\
   & & \ddots & 0 \\
   &   &  & 1 & 0
   \end{pNiceArray} \in \mathfrak{gl}_{n},\]
and let \(E = \begin{pmatrix}0 & 0 \\ 0 & e\end{pmatrix} \in \gl_{m + n}\).
We will show that there is exactly one \(\spl_2\)-triple \((E, H, F)\), and we will find what it looks like.
First we solve the case \(m = 0\) (so \(E = e\)), and then we use this to solve the case of arbitrary \(m\).

\begin{lemma}\label{simple_sl2_triple}
    There is exactly one way to choose \(h, f \in \gl_n\) so that \((e, h, f)\) is an \(\spl_2\)-triple.
\end{lemma}
\begin{proof}
    
Note that \([h', e'] = 2e'\), and \([e', f'] = h'\), and \([h', f'] = -2f'\).
Thus \(e, h, f\) must obey the same relations.
In particular, \(he - eh = 2e\).  
The matrix \(eh\) is \(h\) shifted down one, and \(he\) is \(h\) shifted left one.
Thus, \(2e_{ij} = (he - eh)_{ij} = h_{i, j + 1} - h_{i - 1, j}\).
We can use this to show that \(h_{ij} = 0\) when \(i \neq j\).
Then we can use it to show that \(h_{ii} = h_{i - 1, i - 1} + 2\), so that \(h_{ii} = h_{11} + 2(i - 1)\).
\par Similarly, from \([e, f] = h\) we get that \(h_{ij} = (ef - fe)_{ij} = f_{i - 1, j} - f_{i, j + 1}\).
We can use this to show that \(f_{ij} = 0\) when \(j \neq i + 1\).
Then we can use it to show that \(f_{i,i + 1} = f_{i + 1, i + 2} + h_{i + 1, i + 1}\), that \(f_{1,2} = -h_{1, 1}\), and that \(f_{n - 1, n} = h_{n,n}\).
From the first equation we find that
\[-h_{1,1} = f_{1, 2} = f_{n - 1, n} + \sum_{i = 1}^{n - 1} h_{ii} = \sum_{i = 1}^{n} h_{ii} \implies\]
\[\sum_i h_{ii} = 0.\]
Remark: this is just the statement that \(h \in \spl_n\); in other words, we will see that every choice of \(\spl_2\)-triple in \(\gl_{m + n}\) is also an \(\spl_2\)-triple in \(\spl_{m + n}\).
This shows that \(h_{11} = n - 1, h_{22} = n - 3, ..., h_{nn} = 1 - n\).
So we have determined \(h\); it is 
\[h = \begin{pmatrix}
    n - 1  & \\
    & n - 3 \\
    & & \ddots \\
    & & & 3 - n \\
    & & & & 1 - n 
\end{pmatrix}.\]
Now, we can use our expression for \(f\) in terms of \(h\) to obtain
\[f = \begin{pmatrix}
    0 & 1 - n \\
    & 0 & (1 - n) + (3 - n) \\
    & & \ddots & \ddots \\
    & & & 0 & (1 - n) + \cdots + (n - 1) \\
    & & & & 0
\end{pmatrix} = \]
\[\begin{pmatrix}
    0 & 1 (1 - n) \\
      & 0 & 2 (2 - n) \\
    & & 0 & (n - 2) (-2) \\
    & & & 0 & (n - 1) (-1)\\
    & &  & & 0
\end{pmatrix}.\]
\end{proof}

\begin{lemma}\label{sl2_triple}
    There is exactly one way to choose \(H, F \in \gl_{m + n}\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
\end{lemma}
\begin{proof}
Suppose we have \(H, F\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
Writing \(F =: \begin{pmatrix}
    F_{11} & F_{12} \\
    F_{21} & f
\end{pmatrix}\), and similarly for \(H\), we have
\[2E = [H, E] = \begin{pmatrix}
    0 & H_{12}e \\
    0 & he
\end{pmatrix} - \begin{pmatrix}
    0 & 0 \\
    e H_{21} & eh
\end{pmatrix},\]
\[H = [E, F] = \begin{pmatrix}
    0 & 0 \\
    e F_{21} & ef
\end{pmatrix} - \begin{pmatrix}
    0 & F_{12}e \\
    0 & fe
\end{pmatrix}.\]
From these we observe that \((e, h, f)\) must also be an \(\spl_2\)-triple, so \(h, f\) must be as in \cref{simple_sl2_triple}.
We also see that \(H_{11} = 0\).
Recalling that left multiplication by \(e\) is a down-shift, and right multiplication is a left-shift, we see that \(H_{12}\) is all zeroes except for the leftmost column, and \(H_{21}\) is all zeroes except for the bottom row.
% Then \(F_{12}\) must be all zeros except for the two leftmost columns, and \(F_{21}\) must be all zeroes except for the bottom two rows.
Then our final constraint is that 
\[-2F = [H, F] = \]
\[\begin{pmatrix}
    H_{12} F_{21} & H_{12} F_{22} \\
    H_{21} F_{11} + H_{22} F_{21} & H_{21} F_{12} + H_{22} F_{22}
\end{pmatrix} - \begin{pmatrix}
    F_{12} H_{21} & F_{11} H_{12} + F_{12} H_{22} \\
    F_{22} H_{21} & F_{21} H_{12} + F_{22} H_{22}
\end{pmatrix}.\]
Now \(H_{12} = F_{12}e\), and \(H_{21} = eF_{21}\), from the equation \(H = [E, F]\).
Substituting in the equation above then,
\[-2F = \begin{pmatrix}
    F_{12} e F_{21} & F_{12}e f \\
    eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    F_{12} e F_{21} & F_{11} F_{12}e + F_{12} h \\
    f eF_{21} & F_{21} F_{12}e + fh
\end{pmatrix} =\]
\[\begin{pmatrix}
    0 & F_{12}(fe + h) \\
    eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    0 & F_{11} F_{12}e + F_{12} h \\
    (ef - h)F_{21} & F_{21} F_{12}e + fh
\end{pmatrix} =\]
\[\begin{pmatrix}
    0 & F_{12}fe \\
    eF_{21} F_{11} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    0 & F_{11} F_{12}e \\
    ef F_{21} & F_{21} F_{12}e + fh
\end{pmatrix}.\]
Now we see that \(F_{11} = 0\), and consequently that \(F_{12} = F_{21} = 0\) as well.
This shows that \(H_{12} = H_{21} = 0\).
We conclude that \(H\) and \(F\) just have \(h\) and \(f\) in their bottom-right corners, respectively.
\end{proof}

\section{Finding the Slodowy slices with the same \(E\)}
First we find \(\ker \ad_f\).
We have \((fX)_{ij} = i (n - i) A_{i + 1,j}\), and \((Xf)_{ij} = (j - 1) (n - (j - 1)) A_{i, j - 1}\).
So, for all \(i, j \in \{1, ..., n\}\), we have 
\[i(n - i)A_{i + 1,j} = (j - 1)(n - (j - 1))A_{i, j - 1}.\]
Taking \(j = 1\), we find that \(A_{i,1} = 0\) for \(i \geq 2\).
Then, taking \(j > 1\), we find that for \(i, j \in \{1, ..., n - 1\}\),
\[A_{i + 1, j + 1} = \frac{(j - 1)(n - (j - 1))}{i(n - i)} A_{ij}.\]
So, \(\ker\ad_f\) is the set of matrices which are upper triangular and satisfy the above condition.
\par To find the Slodowy slice associated to the previous \(\spl_2\)-triple \((E, H, F)\), we just need to find \(\ker\ad_F\).
We have 
\[[F, X] = \begin{pmatrix}
    0 & 0 \\
    f X_{21} & fX_{22}
\end{pmatrix} - \begin{pmatrix}
    0 & X_{12} f\\
    0 & X_{22} f
\end{pmatrix}.\]
Thus, \(X_{21}\) must be all zeroes except for the first row, and \(X_{12}\) must be all zeroes except for the last column, and \(X_{22} \in \ker\ad_f\).
There is no restriction on \(X_{11}\).
This describes \(\ker\ad_F\).
\par For \(X \in \mathcal{S}_{m,n}\), define \(u(X) := X_{11}\).

\subsection{Finding \(\widetilde{\mathcal{N}}_{m,n}\)}
Let \(\mathcal{N}_m \subseteq \gl_m\) be the nilpotent elements.
Let \(\mathcal{S}_{m,n}'\) be the set of \(X \in \mathcal{S}_{m,n}\) such that both \(X\) and \(u(X)\) are nilpotent.
Let \(\widetilde{\mathcal{N}}_{m,n} = \{(\mathfrak{b}, X) : X \in \mathfrak{b}\} \subseteq \mathcal{B}_{m + n} \times \mathcal{S}_{m,n}'\).
Define \(\pi_{m,n} : \widetilde{\mathcal{N}}_{m,n} \to \mathcal{N}_m\) by \((\mathfrak{b}, X) \mapsto X_{11}\).
For \(Y \in \gl_m\), we call \(\pi_{m,n}^{-1}(Y)\) the \emph{Springer fiber at the \(n\)-Slodowy slice at \(Y\)}.

\begin{lemma}
    Let \(J\) be a jordan block with zeroes along the diagonal, and let \(A\) be upper triangular and nonzero.
    Then \(J + A\) is not nilpotent.
\end{lemma}
\begin{proof}
    It is straightforward to show by induction that if \(v_i = 0\) for \(i < j\), and \(v_j \neq 0\), then \(((J + A)^k v_j)_{j + k} = v_j\).
    Let \(i\) be such that \(Ae_i \neq 0\).
    Then \((J + A)^{i - 1} e_1\) has nonzero \(e_i\)-component.
    Then \((J + A)^i e_1\) has some nonzero \(e_{i'}\)-component for some \(i' \leq i\).
    Then \((J + A)^{i + (n - i')} e_1\) has some nonzero \(e_n\)-component.
    And \(i + (n - i') \geq n\), so we're done.
\end{proof}

\begin{lemma}
    Let \(X \in \gl_m\), and let 
    \[Y = \begin{pmatrix}
        y_{11} & y_{12} & y_{13} & \cdots & y_{1,n-1} & y_{1n} \\
        d_1 & y_{22} & y_{23} & \cdots & y_{2,n-1} & y_{2n} \\
            & d_2   & y_{33} & \cdots & y_{3,n-1} & y_{3n} \\
            & & \ddots & \cdots & \vdots  & \vdots \\
            & & & d_{n - 2} & y_{n-1,n-1} & y_{n - 1,n}\\
            & & & & d_{n - 1} & y_{nn}
    \end{pmatrix} \in \gl_n.\]
    For any \(a, b \in \mathbb{C}^m\),
    \[\det 
    \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray} = \]
       \[\det X \det Y + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
            & & & \vert \\
            & X & & b     \\
            & & & \vert \\
            \hline 
        \text{---} & a & \text{---} & 0
       \end{pNiceArray}\]
\end{lemma}
\begin{proof}
    By induction on \(n\).
    In the case \(n = 1\), expanding along the last row (taking the usual interpretation of the empty product) gives the desired result.
    \par Now suppose \(n > 1\).
    Expanding along the last row, we get 
    \[d_{n - 1} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n-1} & \\
       & & &  \\
       \end{pNiceArray} -
    y_{nn} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \\
        & X & & & &   \\
        & & & & & \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n} & \\
       & & &  \\
       \end{pNiceArray}.\]
    Using our inductive hypothesis for the first determiniant, and using that \(\det \begin{pNiceArray}{c|c}
        A_{11} & 0 \\
        A_{21} & A_{22}
    \end{pNiceArray} = \det A_{11} \det A_{22}\) for the second, the expression becomes
    \[d_{n - 1} \left(\det X \det Y_{n,n-1} + \left(\prod_{i \leq n - 2} d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}\right)
   - y_{nn} \det X \det Y_{nn} = \]
   \[(d_{n - 1} Y_{n,n-1} - y_{nn} \det Y_{nn})\det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray} = \]
   \[\det Y \det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}.\]
\end{proof}

\begin{corollary}
    If \(X\) is nilpotent, and
    \[\begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray}\]
    is nilpotent  as well, then \(Y\) is nilpotent (TODO: and that other determinant is zero).
    \begin{proof}
        By the previous lemma, the characteristic polynomial of the big matrix is
        \[g_X(\lambda) g_Y(\lambda) + f(\lambda),\]
        where \(g_X(\lambda) = \lambda^m\) is the characteristic polynomial of \(X\), and \(f(\lambda)\) is some polynomial of degree at most \(m - 1\).
    \end{proof}
\end{corollary}
\par Now, taking the previous corolloary and the first lemma together, we see that 
\[\mathcal{S}_{m,n}' = \left\{\begin{pNiceArray}{ccc|cccc}
    & & & & & & \vert \\
    & X & & & & & b    \\
    & & & & & & \vert \\
   \hline
   \text{---} & a & \text{---} & 0 \\
    & & & 1 & 0  & \\
    &  & & & \ddots & \ddots  \\
    & & & & & 1 & 0
   \end{pNiceArray} : a, b \in \mathbb{C}^m, X \in \gl_m \textrm{ is nilpotent}\right\}.\]



\section{Simplifying the definition of a Springer fiber at a Slodowy slice}
\par Let \(\mathcal{N}_m \subseteq \gl_m(\mathbb{C})\) be the subset consisting of nilpotent elements.
For \(X \in \mathcal{N}_m\) and \(a, b \in \mathbb{C}^m\), let
\[A_{X, a, b} = \begin{pNiceArray}{ccc|ccccc}
 & & & & & & & \vert \\
 & X & & & & & & b    \\
 & & & & & & & \vert \\
\hline
\text{---} & a & \text{---} & 0 \\
& & & 1 & 0 \\
& & &  & 1 & \ddots \\
& & &  &   & \ddots & 0 \\
& & &  &   &  & 1 & 0
\end{pNiceArray} \in \mathfrak{gl}_{m + n}(\mathbb{C}).\]
By the definition given in the previous section, we have 
\[\widetilde{\mathcal{N}}_{m, n} = \{(\mathfrak{b}, X) : X \in \mathfrak{b}\} \subseteq \mathcal{B}_{m + n} \times \mathcal{S}_{m,n}',\]
and the Springer fiber at the \(n\)-Slodowy slice at a nilpotent \(X \in \gl_m\) is
\[\pi_{m,n}^{-1}(X) = \{(\mathfrak{b}, A_{X, a, b}) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \in \mathcal{S}_{m, n}'\} \] 
\[= \{(\mathfrak{b}, A_{X, a, b}) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \textrm{nilpotent}\}\]
\[\cong \{(\mathfrak{b}, a, b) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \textrm{ nilpotent}\}.\]
We will make one last simplification to this by using a correspondence between Borel subalgebras and complete flags.
TODO: fix the next few paragraphs, they are out of context.
\par We have \(\mathcal{M} = \{AHA^{-1} : A \in \GL_{m + n}(\mathbb{C})\}\), where \(H \subseteq \gl_{m + n}(\mathbb{C})\) is the set of upper triangular matrices.
We say a map \(X : \gl_{m + n} \to \gl_{m + n}\) preserves a flag \(V_0 \subseteq \cdots \subseteq V_{m + n}\) if \(XV_i \subseteq V_i\) for each \(i\).
Let \(E_0 \subseteq \cdots \subseteq E_{m + n}\) be the standard flag of \(\mathbb{C}^{m + n}\).
Since \(H\) is the set of \(X\) which preserve \(E\),
\[\mathcal{M} = \{\{X : \forall i. \; X(AE_i) \subseteq AE_i\} : A \in \GL_n(\mathbb{C})\}.\]
So, for \(X \in \mathcal{N}\),
\[\pi^{-1}(X) \cong \{(V, a, b) : \forall i. \; A_{X, a, b} V_i \subseteq V_{i - 1}\}.\]

\section{Strategy and setup for finding the irreducible components of a Springer fiber at a Slodowy slice}
\par Fix any \(X \in \mathcal{N}\).
As we have fixed \(X\), we now write \(A_{a, b} := A_{X, a, b}\).
Let \((e_{ij})_{1 \leq i \leq k, 1 \leq j \leq \lambda_i}\) be a Jordan basis for \(X\).
For convenience we define \(e_{i0} := 0\); now we may express the fact that \((e_{ij})\) is a Jordan basis by writing \(\forall i. Xe_{ij} = e_{i,j - 1}\).
\par In this section we find the irreducible components of
\[\pi^{-1}(X) \cong V := \{(A_{a, b}, U) : \forall i. \; A_{a, b} U_{i + 1} \subseteq U_i\}.\]
\par For \(1 \leq w \leq k\) and \(0 \leq r \leq \lambda_w\) (note that we allow \(r = 0\)), define 
\[V_{w,r} := \{(A_{a, b}, U) \in V : \exists P \in \GL_{m}. \; \exists b'. \; (P^{-1}, I_n) A_{a, b} (P, I_n) = A_{e_{wr}, b'} \}.\]

\begin{lemma}
    \(V = \bigcup_{1 \leq w \leq k, 0 \leq r \leq \lambda_{w}} V_{w,r}\).
    Further, \(V_{w_1r_1} = V_{w_2r_2}\) exactly when either \(r_1 = r_2 = 0\), or \(\lambda_{w_1} = \lambda_{w_2} \land r_1 = r_2\).
    When \(V_{w_1r_1} \neq V_{w_2r_2}\), we have \(V_{w_1r_1} \cap V_{w_2r_2} = \emptyset\).
\end{lemma}
\begin{proof}
    TODO
\end{proof}
Now, fix any \(w\) and \(r\).
We will find the irreducible components of \(V_{w,r}\).
These will all happen to be equidimensional (with dimensions independent of \(w\) and \(r\)), so their closures in \(V\) will be the irreducible components of \(V\).
\par Let 
\[G := \{P \in \GL_m : P^{-1} X P = X\},\]
and
\[G_{wr} = \{A \in G : e_{wr}A = e_{wr}\}.\]
\par Now, define 
\[U_{wr} = \{(A_{e_{wr}, b}, U) \in V_{wr}\}.\]
Let \(G\) act on \(V_{wr}\) by
\[P \cdot (A_{e_{wr},b}, U) := ((P, I_n) A_{e_{wr}, b} (P, I_n)^{-1}, (P, I_n)U) = (A_{e_{wr} P^{-1}, Pb}, (P, I_n) U).\]
\par Consider the map \(\varphi : U_{wr} \times G \to V_{wr}\) defined by
\[(x, P) \mapsto P \cdot x.\]
By restriction of \(G\) to \(G_{wr}\) and \(V_{wr}\) to \(U_{wr}\), we obtain an action of \(G_{wr}\) on \(U_{wr}\).
Then, letting \(G_{wr}\) act on \(G\) by \(g \cdot h := hg^{-1}\), we obtain an action of \(G_{wr}\) on \(U_{wr} \times G\).
\begin{lemma}
    As an algebraic variety, \(G\) is irreducible.
\end{lemma}
\begin{proof}
    It's just \(\mathbb{C}^{something}\), since its blocks are the lower-left-toeplitz matrices.
\end{proof}
\begin{lemma}
    % The map \(\varphi\) is a geometric quotient of \(U_{wr} \times G\) by the action of the algebraic group \(G_{wr}\).
    The map \(\varphi\) is some sort of quotient by the action of \(G_{wr}\).
    % The map \(X \mapsto \overline{\varphi(X \times G)}\) is a bijection from the irreducible components of \(U_{wr}\) to the irreducible components of \(V_{wr}\).
\end{lemma}
\begin{proof}
    Let \(y \in V_{wr}\).
    By definition of \(V_{wr}\), there is \(P_y \in G\) with \(P_y \cdot y \in U_{wr}\).
    We have
    \[\varphi^{-1}(y) = \{(x, P) : P \cdot x = y = P_y^{-1} \cdot (P_y \cdot y)\} = \]
    \[\{(x, P) : x = (P^{-1} P_y^{-1}) \cdot (P_y \cdot y) \} = \]
    \[\{((P^{-1}P_y^{-1}) \cdot (P_y \cdot y), P) : (P^{-1} P_y^{-1}) \in G_{wr}\} = \] 
    \[(\textrm{taking } Q := P^{-1}P_y^{-1}, \textrm{ so } P = (QP_y)^{-1} = P_y^{-1} Q^{-1})\]
    \[\{(Q \cdot (P_y \cdot y), P_y^{-1} Q^{-1}) : Q \in G_{wr}\} = \]
    \[\{Q \cdot (P_y \cdot y, P_y^{-1}) : Q \in G_{wr}\}.\]
    So we see that the fibers are exactly the \(G_{wr}\)-orbits.
    \par Now, what kind of quotient is this?
    All I need is something that allows me to deduce the dimension of the quotient...
    % Because \(X\) and \(G\) are irreducible, \(X \times G\) is irreducible, and therefore \(\varphi(X \times G)\) is, as well.
    % Let \(X_1, ..., X_k\) be the irreducible components of \(U_{wr}\).
    % It is clear that \(V_{mr} = \bigcup_i \overline{\varphi(X_i \times G)}\).
    % And if we take \(x \in X_i \setminus \bigcup_{j \neq i} X_j\), it is clear (since \(G\) acts faithfully on itself) that \(\varphi(x, I_m)\)
    % All that remains is to show that \(X_i \notin \bigcup_{j \neq i} \overline{\varphi(X_i \times G)}\).
    % Indeed, 
\end{proof}
Our strategy is to find the irreducible components \(X \subseteq U_{wr}\), and we will then argue that the irreducible components of \(V_{wr}\) are of the form \(\varphi(X \times G)\).
So, we will now find the irreducible components of \(U_{wr}\).
\par Actually this will be unnecessarily difficult to think about; it is easiest in the case \(r = 0\).
So, we will change basis to make \(r = 0\).
Let \(\lambda' = (\lambda_1, ..., \lambda_{w - 1}, \lambda_w - r, \lambda_{w + 1}, ..., \lambda_k)\).
Let \(X'\) be in Jordan normal form with shape \(\lambda'\).
Let \(U_{wr}' = \{(A_{X', 0, b}, U) : \forall i. A_{X', 0, b} U_{i + 1} \subseteq U_i\}\).
Let \(e_{ij}'\) be a Jordan basis for \(X'\).
Let \(f_1, ..., f_n\) be a Jordan basis for the restriction of \(A_{X, 0, b}\) to \(\mathbb{}\)...
Set \(m' := m - r\), and \(n' := n + r\).
Let \(f_1', ..., f_{n'}'\) blah.
Define the linear map \(Q_{wr} : \mathbb{C}^{m' + n'} \to \mathbb{C}^{m + n}\) by:
\begin{itemize}
    \item For all \(i\), \(e_{ij}' \mapsto e_{ij}\).
    \item For \(j = 1, ..., r\), \(f_{n + j}' \mapsto e_{w,(\lambda_w - r) + j}\).
    \item For \(j = 1, ..., n\), \(f_j' \mapsto f_j + e_{w, (\lambda_w - r) - n + j}\).
\end{itemize}
% The inverse map \(Q_{wr}^{-1} : \mathbb{C}^{m + n} \to \mathbb{C}^{m' + n'}\) is
% \begin{itemize}
%     \item For all \(i \neq w\), \(e_{ij} \mapsto e_{ij}'\).
%     \item For \(j = 1, ..., \lambda_w - r\), \(e_{wj} \mapsto e_{wj}'\).
%     \item For \(j = 1, ..., r\), \(e_{wj} \mapsto f'_{n + j}\).
%     \item For \(j = 1, ..., n\), \(f_j \mapsto f'_j - e_{w,\lambda_w - n + r}\)
% \end{itemize}
Observe that conjugation by \(Q_{wr}\) maps \(U_{wr}\) to \(U'_{wr}\), and conjugation by \(Q_{wr}^{-1}\) maps \(U'_{wr}\) to \(U_{wr}\).
We conclude that \(U_{wr} \cong U_{wr}'\); so to find the irreducible components of \(U_{wr}\) we just need to find the irreducible components of \(U_{wr}'\).
To clear the context, which is rather cluttered by now, and to avoid writing primes everywhere, we move to a new section.

\section{Finding the irreducible components of \(U_0\)}
Let \(X\) be nilpotent with Jordan basis \((e_{ij}')_{1 \leq i \leq k, 1 \leq j \leq \lambda_i'}\).
Let \(V = \{(A_{a,b}, U) : \forall i. \; A_{a,b} U_{i + 1} \subseteq U\}\).
Let
\[U_0 = \{(A_{0,b}, U) \in V\}.\]
We will find the irreducible components of \(U_0\).
\par We write \(b_{ij}\) to denote the projection of \(b \in \mathbb{C}^m\) onto \(e_{ij}\).
For each row \(i\), let \(p_i(b) = \max\{j : b_{ij} \neq 0\}\) (the maximum of the empty set is zero).
Then set \(q_i(b) = \lambda_i - p_i(b)\).
When it is clear enough from context, we will just write \(p_i\) and \(q_i\) instead of \(p_i(b)\) and \(q_i(b)\).
\par For any \(I \subseteq \{1, ..., k\}\) and any values \((\rho_i)_{i \in I}\) such that (1) \(\rho_i \leq \lambda_i\), (2) \(\rho_i\) is decreasing with \(i\), and (3) \(\lambda_i - \rho_i\) is decreasing with \(i\), define 
\[U_{I, (\rho_i)} := \] 
\[\{(A_{0, b}, U) \in U_0 : \{(p_i, q_i) : i \in I\} = \{(p_i, q_i) : p_i = \max_{j : q_j = q_i} p_j > \max_{j : q_j < q_i} p_j \} \] 
\[\land \forall i \in I. \; p_i = \rho_i \}.\]
\begin{lemma}
    \(U_0 = \bigcup_{I, (\rho_i)} U_{I, (\rho_i)}\), where \(I\) ranges over all subsets of \(\{1, ..., k\}\), and \((\rho_i)\) ranges over all maps \(I \to \mathbb{N}_{>0}\) such that \(\rho_i\) and \(\lambda_i - \rho_i\) are both decreasing with \(i\).
    TODO: something about overlap being small (they're not disjoint).  
    They are distinct.
    Is that good enough for my purposes?
    Ah, I think it is good enough if I instead say that the sets of \(b\)'s are distinct.
\end{lemma}
\begin{proof}
    Let \((A_{0, b}, U) \in U_0\).
    Let \(I' = \{(p_i, q_i) : p_i = \max_{j : q_j = q_i} p_j > \max_{j : q_j < q_i} p_j\}\).
    Then let \(I = \{\max\{i : (p_i, q_i) = (p, q)\} : (p, q) \in I'\}\).
    Choosing the maximal \(i\) is an arbitrary choice; we just have to pick one.
    For \(i \in I\), \(\rho_i := p_i\).
    All we need to do is show that each \(\rho_i\) is positive, and that \(\rho_i\) and \(\lambda_i - \rho_i\) are decreasing.
    Then we get that \((A_{0,b}, U) \in U_{I, (\rho_i)}\), and we are done.
    \par Recall that by convention the maximum of the empty set is zero.
    So, the fact that \(p_i > \max_{j > i} p_j\) tells us that \(\rho_i = p_i\) is positive.
    \par Let \(i, j \in I\) with \(i < j\).
    Since \(\lambda_i \geq \lambda_j\), we have either \(p_i \geq p_j\) or \(q_i \geq q_j\).
    Further, since \((p_i, q_i) \neq (p_j, q_j)\) (clear from definition of \(I\)), either \(p_i > p_j\) or \(q_i > q_j\).
    And for \(i, j \in I'\) we clearly have \(p_i > p_j \iff q_i > q_j\).
    So in either case, both \(p_i > p_j\) and \(q_i > q_j\), and consequently both \(\rho_i > \rho_j\) and \(\lambda_i - \rho_i > \lambda_j - \rho_j\).
\end{proof}

\begin{lemma}
    Here we have an alternative characterization of \(U_{I, (\rho_i)}\).
    Namely, \(p_i = \rho_i\) for \(i \in I\) and for \(i \notin I\), we have \(p_i \leq something\).
\end{lemma}
\begin{proof}
    
\end{proof}
Fix any \(I\) and \((\rho_i)\) satisfying the conditions of Lemma 8.1.
We will find the irreducible components of \(U_{I, (\rho_i)}\), and show that their closures are in fact irreducible components of \(U_0\).
\par I claim that \(A_{0,b}\) has the same shape for every \((A_{0, b}, U) \in U_{I, (\rho_i)}\).
Call this shape \(\mu\).
By finding a algebraic map taking \(b\) to a Jordan basis for \(A_{0,b}\), we will put \(U_{I, (\rho_i)}\) in isomorphism with the product (choice of \(b\)) \(\times\) (Springer fiber at \(J(\mu)\)).
Then we will use our result about the usual Springer fiber at \(J(\mu)\) to find the irreducible components of \(U_{I, (\rho_i)}\).
(I should define this somewhere else, but \(J(\mu)\) just has the blocks in order of nonincreasing size.)

\par TODO: where does this paragraph go?
Let \(W_\mu\) denote the Springer fiber at \(J(\mu)\), and let \((W_{\mu, d})_d\) be the irreducible components.
We remark that the Springer fiber at \(PJ(\mu)P^{-1}\) is \(PW_\mu\), and its irreducible components are \((PW_{\mu, d})_d\).
\par TODO: begin Jordan basis thing in earnest here.
\par Normally we index \(e_{ij}\) so that \(\lambda_i\) is nonincreasing with \(i\).
Now, it will be more convenient to assume our indices are such that \(q_i\) is nonincreasing, so we will do that: from now on, we assume that \(q_i\) is nonincreasing.
\par We list the chains of the Jordan basis in (roughly) order of decreasing length.
First, for each \(i\) such that \(q_i \geq n\), we take the chain of length \(p_i + q_i\) beginning with \(e_{i, p_i + q_i}\).
\par Now we handle \(f_n\).
Let \(P = \max_{i : q_i < n} p_i\).
Set 
\[v = f_n - (A^{n + P}f_n \rightshift n + P).\]
Note that \(A^n f_n = b\), so \(A^{n + P} f_n = A^P b = b \leftshift P\).
By shifting \(b\) left \(P\) times, we zero out all the rows \(i\) where \(q_i < n\).
This ensures that the operation of shifting \(b' \leftshift P\) right \(n + P\) times is invertible by applying \(A\), and thus shifting left, \(n + P\) times.
So, we take the chain of length \(n + P\) beginning with \(v\).
\par Now, it is clear that each \(f_i\) is in the span of the chains we have listed so far.
It is also clear that \(b \leftshift k\) is in the span, for each \(k \geq 0\).
\par Now, we handle the \(i\) with \(q_i < n\).
We take as an inductive hypothesis that for all \(i' < i\) and all \(j\), we have \(e_{i'j}\) in the span of the chains we have already listed.
\par If \(p_i \leq \max_{k > i} p_k\), then we take the chain beginning with \(e_{i,p_i + q_i}\) of length \(p_i + q_i\).
Clearly we then have \(e_{i'j}\) in the span of the chains we have listed, for \(i' \leq i\).
\par Otherwise, we set 
\[v_i = A^{n - q_i} f_n - \sum_{k = 1}^{p_i}b_{i,k} e_{i,k + q_i}.\]
Note that \(A^{q_i} v_i\) is just \(b\) with row \(i\) zeroed out.
Then, \(A^{q_i + \max_{k > i} p_k} v_i\) will have, in addition, rows \(k\) zeroed out, for all \(k > i\).
This ensures that shifting \(A^{q_i + \max_{k > i} p_k} v_i\) right \(q_i + \max_{k > i} p_k\) times will be inverted by applying \(A\), and thus shifting left, \(q_i + \max_{k > i} p_k\) times.
So, we take the chain beginning with \(v_i - (A^{q_i + \max_{k > i} p_k} v_i >> q_i + \max_{k > i} p_i)\), which has length \(q_i + \max_{k > i} p_k\).
\par Now, we want to show that for each \(j\), \(e_{ij}\) is in the span of the chains we've listed so far.
Since \(A^{q_i + \max_{k > i} p_k} v_i\) has rows \(k\) zeroed out for \(k \geq i\), our inductive hypothesis tells us that \(A^{q_i + \max_{k > i} p_k} v_i \rightshift q_i + \max_{k > i} p_k\) is in the span of the chains already listed.
So it suffices to show that the closure of \(\langle v_i \rangle\) under action by \(A\) contains the span of \(e_{ij}\).
And \(A^{n - q_i} f_n\) is also in the span of chains already listed, so it suffices to show that the closure of \(\langle \sum_k b_{ik} e_{i,k + q_i}\rangle \) under action by \(A\) contains \(\langle e_{ij}\rangle\).
And this just follows from the fact that \(b_{i,p_i}\) is nonzero.
\par Now, I've shown that my purported Jordan basis has a large enough span; to show that it is indeed a Jordan basis I just need to count the number of vectors and show that we obtain \(m + n\).
I list the purported basis here:
\begin{itemize}
    \item TODO
\end{itemize}
Indeed, the sum of the lengths is
\[\left[\sum_{i : q_i \geq n + r} (p_i + q_i)\right] + (r + n + \max_{i : q_i < n + r} p_i) + \sum_{i : q_i < n + r} \begin{cases}
    p_i + q_i, & p_i \leq \max_{k > i} p_k \\
    q_i + \max_{k > i} p_i, & p_i > \max_{k > i} p_k
\end{cases} = \]
\[\left[\sum_{i : q_i \geq n + r} (p_i + q_i)\right] + (r + n + \max_{i : q_i < n + r} p_i) + \sum_{i : q_i < n + r} (q_i + \min(p_i, \max_{k > i} p_i)).\]
Let \(i_1 < \cdots < i_s\) be the `peaks': that is, the set \(I_1 = \{i : q_i < n + r \land p_i > \max_{k > i} p_k\}\).
(As I should've mentioned before, we take the max of the empty set to be zero.)
Let \(I_2 = \{i : q_i < n + r\} \setminus I_1\).
Then 
\[\sum_{i : q_i < n + r} (q_i + \min(p_i, \max_{k > i} p_i)) = \sum_{i \in I_2} (p_i + q_i) + \sum_{k = 1}^s (q_{i_k} + p_{i_{k + 1}}).\]
Clearly then, the whole sum is \(m + n\).



\section{Finding the irreducible components of a springer fiber at a slodowy slice}
Apply things from the previous two sections, and conclude.

\section{Finding a centralizer}
For nilpotent \(X\) in Jordan form and \(a\) in `normalized' form (i.e., with at most one nonzero element, which is a one), we will find the centralizer of \(A_{X, a, b}\) in 
\[\{(A, I) : A \in \gl_m\} \subseteq \gl_{m + n}.\]
Note that an element of the form \((A, I)\) commutes with \(A_{X, a, b}\) if and only if \(A\) commutes with \(X\), and \(aA = a\), and \(Ab = b\).
So, we just have to find 
\[\{A \in \gl_m : AX = XA, aA = a, Ab = b\}.\]
We begin by finding 
\[\{A \in \gl_m : AX = XA\}.\]
We write the shape of \(X\) as \(\lambda = (\lambda_1, ..., \lambda_k)\), so that 
\[X = \begin{pmatrix}
    J_{\lambda_1} & \\
    & \ddots \\
    & & J_{\lambda_k}
\end{pmatrix}.\]
Then we write \(A\) as the block matrix
\[A =: \begin{pmatrix}
    A_{11} & \cdots & A_{1k}\\
    \vdots & & \vdots\\
    A_{k1} & \cdots & A_{kk}
\end{pmatrix},\]
where \(A_{ij}\) is a block of size \(\lambda_i \times \lambda_j\).
We have 
\[XA = \begin{pmatrix}
    J_{\lambda_1}A_{11} & \cdots & J_{\lambda_1}A_{1k}\\
    \vdots & & \vdots\\
    J_{\lambda_k}A_{k1} & \cdots & J_{\lambda_k}A_{kk}
\end{pmatrix}, \textrm{ and } AX = \begin{pmatrix}
    A_{11} J_{\lambda_1} & \cdots & A_{1k} J_{\lambda_k}\\
    \vdots & & \vdots\\
    A_{k1} J_{\lambda_1} & \cdots & A_{kk} J_{\lambda_k}
\end{pmatrix}.\]
So, the constraint that \(XA = AX\) is simply saying that 
\[\forall i,j. \; J_{\lambda_i} A_{ij} = A_{ij} J_{\lambda_j}.\]
It's easy to see that left multiplication by \(J_{\lambda_i}\) is just a down-shift by one, and right multiplication by \(J_{\lambda_j}\) is a left-shift by one.
This means that \(A_{ij}\) is constrained to be a ``lower-left-Toeplitz'' matrix.
A Toeplitz matrix is one which is constant along diagonal bands: \(\forall ijk. \; x_{ij} = x_{i + k,j + k}\).
A lower-left-Toeplitz matrix is one which is all zeroes except for the bottom-left corner; that is, the bands start in the bottom-left corner, and continue until hitting the band which includes the lower-right corner or the band which includes the upper-left corner (whichever comes first).
In other words, an \(n \times m\) lower-left-Toeplitz matrix is one which satisfies \(x_{ij} = 0\) for all \(i,j\) with \(i < j\) and all \(i,j\) with \(i - j < m - n\).
In yet other words, a matrix is lower-left-Toeplitz if it is Toeplitz, satisfies \(x_{1j} = 0\) for \(j \neq 1\), and satisfies \(x_{in} = 0\) for \(i \neq m\).
Anyway, it is easy to see that the lower-left-Toeplitz matrices are exactly those matrices \(A\) such that left-shifting \(A\) by one has the same result as down-shifting by one.
\par So, we have found the set 
\[C_1 := \{A \in \gl_m : AX = XA\}.\]
It is just the set of \(A = [A_{ij}]_{ij}\), where each \(A_{ij}\) is lower-left-Toeplitz.
\par Let \(v_{ij}\) be the leftmost column of \(A_{ij}\), so that
\[A_{ij} = \begin{pmatrix}
    v_{ij} \downshift 0 & \cdots & v_{ij} \downshift [\lambda_j - 1]
\end{pmatrix}.\]
Since \(A_{ij}\) is lower-left-Toeplitz, \(v_{ij}\) is of the form 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) can be chosen freely.
Now we determine which matrices of this form satisfy \(aA = a\).
For simplicity, we will instead find the matrices \(A\) such that \(aA = 0\) (so that \(a(A + I) = a\)).
(Note that \(I \in C_1\), and \(C_1\) is closed under addition, so this is really the same thing.)
\par In the case that \(a = 0\), clearly every \(A\) works.
Otherwise, let \(i_0, j_0\) be such that \(a_{i_0,j_0} = 1\).
Now, clearly the constraint that \(aA = 0\) is just saying that the \((i_0,j_0)\)th row of \(A\) must be zero.
That is, for each \(j\) the \(j_0\)th row of \(A_{i_0j}\) must be zero.
This just requires that for each \(j\), we must have 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) can be chosen freely.
So, we have now found the set 
\[C_2 := \{A \in \gl_m : AX = XA, aA = a\}.\]
It is the matrices of the form \(I + A\), where \(A_{ij}\) is a block matrix of size \(\lambda_i \times \lambda_j\) with 
\[A_{ij} = T(v_{ij}) = T\begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) in the case \(i \neq i_0\) and \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) if \(i = i_0\).

\section{A Jordan basis for \(A_{X, a, b}\)}

\subsection{A `normalization' fact about Jordan bases}
Let \(V\) be a finite-dimensional vector space, \(A : V \to V\) a nilpotent operator, and \(f : V \to \mathbb{C}\) a linear map.
Let \((e_{ij} : i \leq m, j \leq \lambda_i)\) be a Jordan basis for \(A\).

\begin{lemma}
    There is a Jordan basis \(e_{ij}\) for \(A\) such that there is at most one pair \((i, j)\) with \(f(e_{ij}) \neq 0\).
\end{lemma}
\begin{proof}
    For any change of basis \(P : V \to V\) commuting with \(A\), we obtain a new Jordan basis \((P(e_{ij}) : i \leq m, j \leq \lambda_i)\).
    For any such \(P\), define 
    \[S_P = \sum_i \begin{cases}-1, & \forall j. \; f \circ P(e_{ij}) = 0 \\ \lambda_i - \min\{j : f \circ P(e_{ij}) \neq 0\}, & \textrm{otherwise} \end{cases}.\]
    Let \(P\) be any operator, among all invertible operators commuting with \(A\), which minimizes \(S_P\).
    Write \(e_{i,j}' := P(e_{i,j})\).
    \par Suppose for contradiction that there are two distinct \(i\)'s (and some \(j\)'s) with \(f(e_{ij}') \neq 0\).
    Then we can take \(e_{i_1, j_1}'\) and \(e_{i_2, j_2}'\), where for \(k \in \{1,2\}\) we have \(f(e_{i_k, j_k}') \neq 0\), and \(\forall j < j_k. \; f(e_{i_k, j}') = 0\).
    Wlog, assume \(\lambda_{i_1} - j_1 \leq \lambda_{i_2} - j_2\).
    Then, we can define \(Q : V \to V\) by
    \begin{itemize}
        \item \(Q(e_{i_1, \lambda_1}') := e_{i_1, \lambda_1}' - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}'\)
        \item For \(j < \lambda_1\), \(Q(e_{i_1, j}') := A^{\lambda_{i_1} - j} Q(e_{i_1, \lambda_{i_1}}')\)
        \item For \(i \neq i_1\), \(Q(e_{ij}') = e_{ij}'\).
    \end{itemize}
    Clearly \(Q\) is invertible, and it commutes with \(A\).
    Further, I claim that \(S_{QP}  < S_P\).
    It suffices to show that \(\forall j \leq j_1. \; f \circ Q(e_{i_1,j}') = 0\).
    We have 
    \[f \circ Q(e_{i_1,j}') = f\left(A^{\lambda_{i_1} - j}\left(e_{i_1, \lambda_1}' - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}'\right)\right) =\]
    \[f\left(e_{i_1, j}' - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})} e_{i_2, j_2 + (j - j_1)}'\right) = f(e_{i_1, j}') - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})} e'_{i_2, j_2 + (j - j_1)}.\]
    Clearly (by design), this expression is zero when \(j = j_1\).
    And for \(j < j_1\), we have \(f(e'_{i_1, j}) = f(e'_{i_2, j_2 + (j_2 - j_1)}) = 0\), so it is zero then as well.
    Thus we see that \(S_{QP} < S_P\), contradicting that \(S_P\) is minimal.
    So there must be at most one \(i\) such that there exists \(j\) such that \(f(e_{ij}') \neq 0\).
    \par If there is no such \(i\), we have found the desired basis.
    So, suppose there is such an \(i_0\).
    Let \(U = \langle e_{i_0, j}' \rangle\).
    Simply write \(e_j := e_{i_0, j}'\).
    Let \(j_0 = \min\{j : f(e_j) \neq 0\}\).
    We just have to change basis to zero out \(e_j\) for \(j \neq j_0\).
    Let \(P_1(e_{\lambda_{i_0}}) := e_{\lambda_{i_0}} - \frac{f(e_{j_0 + 1})}{f(e_{j_0})} e_{\lambda_{i_0} - 1}\), and notice that \(f \circ P_1(e_j) = 0\) for all \(j \leq j_0 + 1\) except for \(j_0\).
    Then we define \(P_2(e_{\lambda_{i_0}}) := P_1(e_{\lambda_{i_0}}) - \frac{f\circ P_1(e_{j_0 + 2})}{f\circ P_1(e_{j_0})} e_{\lambda_{i_0} - 2}\), and notice that \(f \circ P_2(e_j) = 0\) for all \(j \leq j_0 + 2\) except for \(j_0\).    
    Eventually we get \(P_{\lambda_{i_0} - j_0}\), and by applying this to the \(e_j\)'s we obtain a basis of \(U\), in which there is exactly one \(j\) with \(f \circ P_{\lambda_{i_0} - j_0}(e_j) \neq 0\).
\end{proof}

\subsection{When \(X\) is all zeroes}
In this case we have \(A(y, z) = (z_n b, (a \cdot y, z_1, ..., z_{n - 1}))\), and the condition becomes
\[\sum_i b_i a_i = 0.\]
In this case we will be able to write down explicitly the irreducible components of \(F := \{(V, a, b) : \forall i. \; A_{X, a, b} V_i \subseteq V_i\} \cong \pi^{-1}(X)\).
For any nonnegative \(\delta_0, \delta_1, ..., \delta_n, \delta_{n + 1}\) summing to \(m\), define the corresponding sequence \(i_0 = \delta_0\), \(i_n = \delta_{n + 1} + i_n\), and for \(j \in \{1, ..., n\}\), \(i_j = i_{j - 1} + 1 + \delta_j\).
Let \(E\) be the span of the \(e_i\)'s, and let \(E' = \{(x, 0) \in E : x \cdot a = 0\}\), where the dot is the \(m\)-dimensional dot product.
Then define \(F_\delta\) as the set of \((V, a, b) \in F\) such that 
\begin{itemize}
    \item \(b \in V_{i_0} \subseteq E'\)
    \item for all \(j \in \{1, ..., n\}\), we have \(f_j \in V_{i_j} \subseteq E' + \langle f_1, ..., f_j \rangle\)
\end{itemize}
I claim that the \(F_\delta\)'s are the irreducible components of \(F\).
To begin, I show that their union is \(F\).

\begin{lemma}
    Let \((V, a, b) \in \mathcal{F}\).
    Write \(f_0 = b\), and \(F = \langle f_0, f_1, ..., f_n \rangle\).
    For each \(i\), either \(F \subseteq V_i\), or else there exists \(j\) such that \(e_j \notin V_i\), but \(\langle e_0, ..., e_{j - 1} \rangle \subseteq V_i \subseteq E' + \langle e_1, ..., e_j \rangle\).
\end{lemma}
\begin{proof}
    For \(i = 0\), we may take \(j = 0\).
    Now assume the statement holds for \(i\), and we will prove it for \(i + 1\).
    If \(F \subseteq V_i\), then \(F \subseteq V_{i + 1}\), and we are done.
    \par So, suppose there is \(j\) such that \(e_j \notin V_i\), but \(\langle e_0, ..., e_{j - 1} \rangle \subseteq V_i \subseteq E' + \langle e_0, ..., e_j \rangle\).
    We have two cases: either \(V_{i + 1} = V_i + \langle e_j \rangle\), or not.
    \begin{itemize}
        \item If so, then either \(j = n\), in which case \(F \subseteq V_{i + 1}\), or else \(j \neq n\), in which case \(e_{j + 1} \notin V_{i + 1}\), but \(\langle e_0, ..., e_j \rangle \subseteq V_{i + 1} \subseteq E' + \langle e_0, ..., e_{j + 1} \rangle\).
        \item If not, then \(e_j \notin V_{i + 1}\).
        Let \(v_{i + 1}\) be so that \(V_{i + 1} = V_i + \langle v_{i + 1} \rangle\).
        I just need to show that \(v_{i + 1} \in Y + \langle e_1, ..., e_j \rangle\).
        It suffices to show that \(v_{i + 1}^\top e_k = 0\) for \(k > j\).
        And to do this, it suffices to show that \((Av_{i + 1})^\top e_{k - 1} = 0\) for \(k > j\).
        \par Note that \(Av_{i + 1} \in V_i \cap A \mathbb{C}^{2n} \subseteq \langle e_1, ..., e_j \rangle\).
        So, for \(k > j + 1\) it is clear that \((Av_{i + 1})^\top e_{k - 1} = 0\).
        Now, suppose for contradiction that \((Av_{i + 1})^\top e_j \neq 0\).
        Then \(Av_{i + 1}\) is linearly independent of \(e_1, ..., e_{j + 1}\).
        Since \(Av_{i + 1} \in \langle e_1, ..., e_j \rangle\), it follows that \(e_j \in \langle e_1, ..., e_{j - 1}, Av_{i + 1} \rangle \subseteq V_i\), a contradiction.
    \end{itemize}
\end{proof}

\begin{corollary}
    \(\mathcal{F} = \bigcup_\delta \mathcal{F}_\delta\)
\end{corollary}
\begin{proof}
    Let \((V, a, b) \in F\).
    Let \(i_0 = \min\{i : b \in V_i\}\), and let \(i' = \max\{i : V_i \subseteq E'\}\).
    We want that \(b \in V_{i_0} \subseteq E'\), so we want that \(i_0 \leq i'\).
    For contradiction, suppose \(i' < i_0\).

\end{proof}

\begin{lemma}
    Each \(\mathcal{F}_\delta\) is a closed subvariety of \(\mathcal{F}\).
\end{lemma}
\begin{proof}
    
\end{proof}

\begin{lemma}
    Each \(\mathcal{F}_\delta\) is irreducible of dimension \(m\).
\end{lemma}
\begin{proof}
    Let \(\mathcal{E}\) be the variety of partial flags of \(E\) of shape \((\delta_0, ..., \delta_{n + 1})\).
    Define \(g : \mathcal{F}_\delta \to \mathcal{E}\) by
    \[(V, a, b) \mapsto 0 \subseteq V_{i_0} \cap E \subseteq V_{i_1} \cap E \subseteq \cdots \subseteq V_{i_{n + 1}} \cap E = E.\]
    It is clear that \(g\) is surjective, as the definition of \(\mathcal{F}_\delta\) places no restriction on the intersections \(V_i \cap E\).
    Now let's look more closely at the fibers \(g^{-1}(U)\).
    \par First let's find the flags \(V\) such that there exist \(a, b\) with \(g(V, a, b) = U\).
    We see that, for instance, \(V_{i_0} \cap E = U_1\).
    In fact \(V_{i_0} \subseteq E\), so \(V_{i_0} = U_1\).
    But we are free to choose the vector spaces between \(0\) and \(V_{i_0}\) however we wish, so we get some degrees of freedom like \(\mathcal{F}_{\delta_0}\), the complete flag variety on \(\mathbb{C}^{\delta_0} \cong V_{i_0} / 0\).
    Similarly, for every \(j = 1, ..., n + 1\), we can choose the vector spaces between \(V_{i_{j - 1}}\) and \(V_{i_j}\) arbitrarily, so we get degrees of freedom like \(\mathcal{F}_{\delta_{i_j}}\), the complete flag variety on \(\mathbb{C}^{\delta_{i_j}} \cong V_{i_j} / V_{i_{j - 1}}\).
    Finally, to meet the constraint of \((V, a, b)\) being in \(\mathcal{F}_\delta\), we can choose any \(b \in U_1\) and any \(a\) such that \(\overline{a} \in U_n^\perp\).
    Thus we get an isomorphism
    \[g^{-1}(U) \cong \mathcal{F}_{\delta_0} \times \cdots \times \mathcal{F}_{\delta_{n + 1}} \times \mathbb{C}^{\delta_0} \times \mathbb{C}^{\delta_{n + 1}}.\]

\end{proof}

\begin{theorem}
    The \(\mathcal{F}_\delta\)'s are the irreducible components of \(\mathcal{F}\).
\end{theorem}

\subsection{In the case that \(X\) is a Jordan block}
This case seems harder to work with explicitly than the case that \(X\) is zero, so our strategy is to reuse 


\section{A different variety}
Define \(R = \{(X, \mathfrak{b}_1, \mathfrak{b}_2) : X \in \mathfrak{b}_1 \land u(X) \in \mathfrak{b}_2\} \subseteq \mathcal{S}_{m,n}' \times \mathcal{B}_{m + n} \times \mathcal{B}_m\).
We can obtain a subvariety of \(R\) by requiring that \(u(X)\) is in some fixed similarity class.
(TODO: why is this a subvariety?  Is it?  Is this even the right way of explaining the significance of the \(m^2\)?)
We expect that each of these subvarieties is an irreducible component of dimension \(m^2\).
We will verify these things using our prvious computations of springer fibers.

\subsection{TODO}
\begin{itemize}
    \item Why are SOn flags what they are.
\end{itemize}

\bibliography{paper}
\end{document}
 