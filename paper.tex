\documentclass[12pt,psamsfonts]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{blkarray}
\usepackage{upquote}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{url}
\usepackage{tikz-cd}
\usepackage{enumerate}
\usepackage{rotating}

\newcommand{\leftshift}{\,\texttt{<<}\,}
\newcommand{\downshift}{\mathbin{\rotatebox[origin=c]{90}{\leftshift}}}
\newcommand{\rightshift}{\mathbin{\rotatebox[origin=c]{180}{\leftshift}}}
\newcommand{\upshift}{\mathbin{\rotatebox[origin=c]{270}{\leftshift}}}

\newcommand{\deriv}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\stab}{stab}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Image}{Image}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\real}{Re}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\atan2}{atan2}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\Br}{Br}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\inv}{inv}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\gl}{\mathfrak{gl}}
\DeclareMathOperator{\spl}{\mathfrak{sl}}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\un}{\mathrm{un}}
\newcommand{\al}{\mathrm{al}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\comment}{}

\usepackage[capitalize,nameinlink,noabbrev]{cleveref} % to emulate \autoref style


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
% \newcommand{\theoremautorefname}{Theorem}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\definitionautorefname}{Definition}

\bibliographystyle{plain}
\usepackage{nicematrix}

% \author{Owen Conoly}
% \date{Date}
% \title{Title}

\begin{document}

% \maketitle

\section{Introduction}
We will review the definition of a Springer fiber and define, for a nilpotent \(Y \in \gl_m\), the Springer fiber at the \(n\)-Slodowy slice at \(Y\).
For every \(n\) and every nilpotent \(Y \in \gl_m\), we will find the irreducible components of the Springer fiber at the \(n\)-Slodowy slice at \(Y\).
Finally, we will use our results about Springer fibers at \(n\)-Slodowy slices to find the irreducible components of some other variety (which probably needs a name), and show that they all have the same dimension.

\section{Preliminary Definitions and Facts}
\subsection{Conventions and Notations}
We write \(\GL_m, \spl_m\) to denote \(\GL_m(\mathbb{C}), \spl_m(\mathbb{C})\), and so on.
By \(J_m\) we refer to the nilpotent \(m \times m\) Jordan block (which, by convention, has ones \emph{below} the diagonal).
Given a partition \(\lambda = (\lambda_1, ..., \lambda_k)\), we write \(J(\lambda)\) to denote the block matrix
\[\begin{pmatrix}
    J_{\lambda_1} \\
    & \ddots \\
    & & J_{\lambda_k}
\end{pmatrix}.\]



\subsection{Springer fibers}
Let \(G \subseteq \GL_m\) be a connected semisimple Lie group, and let \(\mathfrak{g} \subseteq \gl_m\) be its Lie algebra.
(TODO: either say something interesting about the Lie group, or just get rid of it and talk only about the Lie algebra.)
% ^I take this scope because it is about what Yuzhou Gu said.
Let \(\mathcal{N} \subseteq \mathfrak{g}\) be the subset consisting of nilpotent elements.
Let \(\mathcal{B}\) be the variety of Borel subalgebras of \(\mathfrak{g}\).
Let \(\widetilde{\mathcal{N}} = \{(\mathfrak{b}, n) : n \in \mathfrak{b}\} \subseteq \mathcal{B} \times \mathcal{N}\).
Let \(\pi : \widetilde{\mathcal{N}} \to \mathcal{N}\) be the projection onto the second coordinate.
We call this the \emph{Springer resolution}.
For \(n \in \mathcal{N}\), we call \(\pi^{-1}(n)\) the \emph{Springer fiber at \(n\)}.

\subsection{Springer fibers in \(\gl_m\)}
Now we let \(\mathcal{N}\) be the set of nilpotent elements in \(\gl_m\), and \(\mathcal{B}\) the variety of Borel subalgebras of \(\gl_m\).
Let \(H \subseteq \gl_m\) be the subalgebra of upper triangular matrices.
The variety of Borel subalgebras of \(\gl_m\) is \(\mathcal{B} = \{gHg^{-1} : g \in \GL_m\}\).
Thus, the Springer fiber at \(X \in \mathcal{N}\) is 
\[S_X = \{gHg^{-1} : X \in gHg^{-1}\}.\]
\begin{definition}
    A \emph{flag} \((V_i)\) of \(\mathbb{C}^m\) is a sequence of subspaces
    \[0 = V_0 \subseteq V_1 \subseteq \cdots \subseteq V_m = \mathbb{C}^m,\]
    where \(\dim V_i = i\).
\end{definition}
We say that \(X \in \gl_m\) \emph{preserves} a flag \((V_i)\) if \(\forall i. \; XV_i \subseteq V_i\).
Note that \(X \in \mathcal{N}\) preserves a flag \((V_i)\) if and only if \(\forall i. \; XV_i \subseteq V_{i - 1}\).
\par The simplest flag is the \emph{standard flag} \((E_i)\), where \(E_i := \langle e_1, ..., e_i \rangle\).
Note that the group \(H\) is exactly the subset of \(\gl_m\) which preserves \(E_i\).
\par We can think of \(\mathcal{B}\) as the set of flags of \(\mathbb{C}^m\), via the correspondence 
\[gHg^{-1} \leftrightarrow (gE_i).\]
Note that \(X\) preserves \((gE_i)\) if and only if \(X \in gHg^{-1}\).
Thus, we may write the Springer fiber at \(X \in \mathcal{N}\) in terms of flags, as 
\[S_X = \{(gE_i) : X \in gHg^{-1}\} = \{(V_i) : \forall i. \; XV_i \subseteq V_{i - 1}\}.\]

\begin{theorem}\label{usual_springer_fiber}
    (Needs citation!)
    The irreducible components of the Springer fiber at \(J(\mu)\) are in bijection with the standard Young tableaus of shape \(\mu\).
    Further, the irreducible components are equidimensional, of dimension \(\sum_{i \neq j} \min(\mu_i, \mu_j) = \sum_i (i - 1) \mu_i\).
\end{theorem}

\subsection{Slodowy Slices}

A basis for \(\spl_2\) is
\[e' := \begin{pmatrix}0 & 1 \\ 0 & 0 \\\end{pmatrix}, h' := \begin{pmatrix}1 & 0 \\0 & -1\end{pmatrix}, f' := \begin{pmatrix}0 & 0 \\1 & 0\end{pmatrix}.\]
Given a Lie algebra \(\mathfrak{g}\), and a homomorphism \(\phi : \spl_2 \to \mathfrak{g}\) sending \((e', h', f')\) to \((e, h, f)\), we say that \((e, h, f)\) is an \emph{\(\spl_2\)-triple}.
Observe that \(e, f\) must be nilpotent, and \(h\) must be Cartan (TODO: what?)
If \(\mathfrak{g}\) is semisimple, then given any nilpotent \(e \in \mathfrak{g}\), the Jacobson-Morozov theorem \cite[3.7.1]{ehf} says that there exist \(h, f \in \mathfrak{g}\) such that \((e, h, f)\) is an \(\spl_2\)-triple.
\par Given \((e, h, f)\), we define the \emph{Slodowy slice at \(e\)} as \(\mathcal{S}_e := e + \ker \ad_f\).
By the Jacobson-Morozov theorem, we can always find a Slodowy slice at any nilpotent \(e \in \mathfrak{g}\), when \(\mathfrak{g}\) is semisimple.

\section{Some linear algebra facts}
In this section we prove linear algebra facts that will be useful later.
\subsection{The centralizer of a nilpotent matrix}
\begin{definition}
    A matrix \(Y\) is \emph{Toeplitz} if it is constant along bands parallel to the main diagonal.
    That is, \(\forall i,j,k. \; Y_{ij} = Y_{i + k, j + k}\).
\end{definition}
\begin{definition}
    An \(m \times n\) matrix \(Y\) is \emph{lower-left Toeplitz} if it is Toeplitz and, in addition, we have \(y_{n - i, j - 1} = 0\) whenever \(i + j \geq \min(m,n)\).
\end{definition}
That is, \(Y\) is lower-left Toeplitz if it is Toeplitz, and the only nonzero entries are those with Manhattan distance less than \(\min(m,n)\) from the lower-left corner.
In yet other words, only the leftmost \(\min(m,n)\) diagonal bands are nonzero.

\begin{lemma}
    Let \(\lambda = (\lambda_1, ..., \lambda_k)\) be a partition of \(m\).
    The centralizer of \(J(\lambda)\) in \(\gl_m\) is the subalgebra consisting of matrices 
    \[M = \begin{pmatrix}
        M_{11} & \cdots & M_{1k}\\
        \vdots &        & \vdots\\
        M_{k1} & \cdots & M_{kk}
    \end{pmatrix},\]
    where each \(M_{ij}\) is a \(\lambda_i \times \lambda_j\) matrix, such that each \(M_{ij}\) is lower-left Toeplitz.
\end{lemma}
\begin{proof}
    Let 
    \[M = \begin{pmatrix}
        M_{11} & \cdots & M_{1k}\\
        \vdots &        & \vdots\\
        M_{k1} & \cdots & M_{kk}
    \end{pmatrix}.\]
    We need to show that \(J(\lambda)M = MJ(\lambda)\) if and only if each \(M_{ij}\) is lower-left Toeplitz.
    \par We have 
    \[J(\lambda)M = \begin{pmatrix}
        J_{\lambda_1}M_{11} & \cdots & J_{\lambda_1}M_{1k}\\
        \vdots & & \vdots\\
        J_{\lambda_k}M_{k1} & \cdots & J_{\lambda_k}M_{kk}
    \end{pmatrix}, \textrm{ and } MJ(\lambda) = \begin{pmatrix}
        M_{11} J_{\lambda_1} & \cdots & M_{1k} J_{\lambda_k}\\
        \vdots & & \vdots\\
        M_{k1} J_{\lambda_1} & \cdots & M_{kk} J_{\lambda_k}
    \end{pmatrix}.\]
    So, we have \(J(\lambda)M = MJ(\lambda)\) if and only if \(\forall i,j. \; J_{\lambda_i} M_{ij} = M_{ij} J_{\lambda_j}\).
    Multiplying on the left by \(J_{\lambda_i}\) just shifts each row down by one, and multiplying on the right by \(J_{\lambda_j}\) shifts each column left by one.
    The matrices for which left-shifting gives the same result as down-shifting are exactly the lower-left Toeplitz matrices.
\end{proof}
For nilpotent \(X\) in Jordan form and \(a\) in `normalized' form (i.e., with at most one nonzero element, which is a one), we will find the centralizer of \(A_{X, a, b}\) in 
\[\{(A, I) : A \in \gl_m\} \subseteq \gl_{m + n}.\]
Note that an element of the form \((A, I)\) commutes with \(A_{X, a, b}\) if and only if \(A\) commutes with \(X\), and \(aA = a\), and \(Ab = b\).
So, we just have to find 
\[\{A \in \gl_m : AX = XA, aA = a, Ab = b\}.\]

It is just the set of \(A = [A_{ij}]_{ij}\), where each \(A_{ij}\) is lower-left-Toeplitz.
\par Let \(v_{ij}\) be the leftmost column of \(A_{ij}\), so that
\[A_{ij} = \begin{pmatrix}
    v_{ij} \downshift 0 & \cdots & v_{ij} \downshift [\lambda_j - 1]
\end{pmatrix}.\]
Since \(A_{ij}\) is lower-left-Toeplitz, \(v_{ij}\) is of the form 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) can be chosen freely.
Now we determine which matrices of this form satisfy \(aA = a\).
For simplicity, we will instead find the matrices \(A\) such that \(aA = 0\) (so that \(a(A + I) = a\)).
(Note that \(I \in C_1\), and \(C_1\) is closed under addition, so this is really the same thing.)
\par In the case that \(a = 0\), clearly every \(A\) works.
Otherwise, let \(i_0, j_0\) be such that \(a_{i_0,j_0} = 1\).
Now, clearly the constraint that \(aA = 0\) is just saying that the \((i_0,j_0)\)th row of \(A\) must be zero.
That is, for each \(j\) the \(j_0\)th row of \(A_{i_0j}\) must be zero.
This just requires that for each \(j\), we must have 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) can be chosen freely.
So, we have now found the set 
\[C_2 := \{A \in \gl_m : AX = XA, aA = a\}.\]
It is the matrices of the form \(I + A\), where \(A_{ij}\) is a block matrix of size \(\lambda_i \times \lambda_j\) with 
\[A_{ij} = T(v_{ij}) = T\begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) in the case \(i \neq i_0\) and \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) if \(i = i_0\).

\subsection{A `normalization' fact about Jordan bases}
Let \(V\) be a finite-dimensional vector space, \(A : V \to V\) a nilpotent operator, and \(f : V \to \mathbb{C}\) a linear map.
% Let \((e_{ij} : i \leq m, j \leq \lambda_i)\) be a Jordan basis for \(A\).

\begin{lemma}
    There is a Jordan basis \(e_{ij}\) for \(A\) such that there is at most one \(i\) such that there exists \(j\) such that \(f(e_{ij}) \neq 0\).
\end{lemma}
\begin{proof}
    For any Jordan basis \((e_{ij})_{ij}\) of \(A\), define
    \[S((e_{ij})_{1 \leq i \leq k, 1 \leq j \leq \lambda_i}) := \sum_i \begin{cases}-1, & \forall j. \; f (e_{ij}) = 0 \\ \lambda_i - \min\{j : f (e_{ij}) \neq 0\}, & \textrm{otherwise} \end{cases}.\]
    We proceed by induction on the measure \(S\).
    That is, let \((e_{ij})_{ij}\) be a Jordan basis for \(A\).
    Our inductive hypothesis is that if there exists a Jordan basis \((e_{ij}')_{ij}\) with \(S((e_{ij}')_{ij}) < S((e_{ij})_{ij})\), then we get the desired conclusion.
    \par Now, we have two cases.
    In the first case, \((e_{ij})_{ij}\) already satisfies the desired property.
    In this case we are done.
    In the other case, there exist \(i_1, j_1, i_2, j_2\) with \(f(e_{i_1j_1}) \neq 0\) and \(f(e_{i_2j_2}) \neq 0\).
    We let \(j_1, j_2\) be minimal with this property, so that \(\forall j < j_1. \; f(e_{i_1j}) = 0\), and \(\forall j < j_2. \; f(e_{i_2j}) = 0\).
    Wlog, we assume that \(\lambda_{i_1} - j_1 \leq \lambda_{i_2} - j_2\).
    \par By our inductive hypothesis, all we need to do is find a Jordan basis \((e_{ij}')_{ij}\) with \(S((e_{ij}')_{ij}) < S((e_{ij})_{ij})\).
    This is what we do.
    Define \(e'_{ij}\) as follows.
    \begin{itemize}
        \item \(e_{i_1, \lambda_1}' := e_{i_1, \lambda_1} - \frac{f(e_{i_1, j_1})}{f(e_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}\)
        \item CONTINUE HERE
        \item For \(j < \lambda_1\), \(Q(e_{i_1, j}') := A^{\lambda_{i_1} - j} Q(e_{i_1, \lambda_{i_1}}')\)
        \item For \(i \neq i_1\), \(Q(e_{ij}') = e_{ij}'\).
    \end{itemize}
    Clearly \(Q\) is invertible, and it commutes with \(A\).
    Further, I claim that \(S_{QP}  < S_P\).
    It suffices to show that \(\forall j \leq j_1. \; f \circ Q(e_{i_1,j}') = 0\).
    We have 
    \[f \circ Q(e_{i_1,j}') = f\left(A^{\lambda_{i_1} - j}\left(e_{i_1, \lambda_1}' - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}'\right)\right) =\]
    \[f\left(e_{i_1, j}' - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})} e_{i_2, j_2 + (j - j_1)}'\right) = f(e_{i_1, j}') - \frac{f(e'_{i_1, j_1})}{f(e'_{i_2, j_2})} e'_{i_2, j_2 + (j - j_1)}.\]
    Clearly (by design), this expression is zero when \(j = j_1\).
    And for \(j < j_1\), we have \(f(e'_{i_1, j}) = f(e'_{i_2, j_2 + (j_2 - j_1)}) = 0\), so it is zero then as well.
    Thus we see that \(S_{QP} < S_P\), contradicting that \(S_P\) is minimal.
    So there must be at most one \(i\) such that there exists \(j\) such that \(f(e_{ij}') \neq 0\).
    \par If there is no such \(i\), we have found the desired basis.
    So, suppose there is such an \(i_0\).
    Let \(U = \langle e_{i_0, j}' \rangle\).
    Simply write \(e_j := e_{i_0, j}'\).
    Let \(j_0 = \min\{j : f(e_j) \neq 0\}\).
    We just have to change basis to zero out \(e_j\) for \(j \neq j_0\).
    Let \(P_1(e_{\lambda_{i_0}}) := e_{\lambda_{i_0}} - \frac{f(e_{j_0 + 1})}{f(e_{j_0})} e_{\lambda_{i_0} - 1}\), and notice that \(f \circ P_1(e_j) = 0\) for all \(j \leq j_0 + 1\) except for \(j_0\).
    Then we define \(P_2(e_{\lambda_{i_0}}) := P_1(e_{\lambda_{i_0}}) - \frac{f\circ P_1(e_{j_0 + 2})}{f\circ P_1(e_{j_0})} e_{\lambda_{i_0} - 2}\), and notice that \(f \circ P_2(e_j) = 0\) for all \(j \leq j_0 + 2\) except for \(j_0\).    
    Eventually we get \(P_{\lambda_{i_0} - j_0}\), and by applying this to the \(e_j\)'s we obtain a basis of \(U\), in which there is exactly one \(j\) with \(f \circ P_{\lambda_{i_0} - j_0}(e_j) \neq 0\).
\end{proof}


\section{Finding \(\spl_2\)-triples \((E, H, F)\) in \(\gl_{m + n}\) with a particular \(E\).}
\par Let
\[e = \begin{pNiceArray}{ccccc}
 0 \\
    1 & 0 \\
  & 1 & \ddots \\
   & & \ddots & 0 \\
   &   &  & 1 & 0
   \end{pNiceArray} \in \mathfrak{gl}_{n},\]
and let \(E = \begin{pmatrix}0 & 0 \\ 0 & e\end{pmatrix} \in \gl_{m + n}\).
We will show that there is exactly one \(\spl_2\)-triple \((E, H, F)\), and we will find what it looks like.
First we solve the case \(m = 0\) (so \(E = e\)), and then we use this to solve the case of arbitrary \(m\).

\begin{lemma}\label{simple_sl2_triple}
    There is exactly one way to choose \(h, f \in \gl_n\) so that \((e, h, f)\) is an \(\spl_2\)-triple.
\end{lemma}
\begin{proof}
    
Note that \([h', e'] = 2e'\), and \([e', f'] = h'\), and \([h', f'] = -2f'\).
Thus \(e, h, f\) must obey the same relations.
In particular, \(he - eh = 2e\).  
The matrix \(eh\) is \(h\) shifted down one, and \(he\) is \(h\) shifted left one.
Thus, \(2e_{ij} = (he - eh)_{ij} = h_{i, j + 1} - h_{i - 1, j}\).
We can use this to show that \(h_{ij} = 0\) when \(i \neq j\).
Then we can use it to show that \(h_{ii} = h_{i - 1, i - 1} + 2\), so that \(h_{ii} = h_{11} + 2(i - 1)\).
\par Similarly, from \([e, f] = h\) we get that \(h_{ij} = (ef - fe)_{ij} = f_{i - 1, j} - f_{i, j + 1}\).
We can use this to show that \(f_{ij} = 0\) when \(j \neq i + 1\).
Then we can use it to show that \(f_{i,i + 1} = f_{i + 1, i + 2} + h_{i + 1, i + 1}\), that \(f_{1,2} = -h_{1, 1}\), and that \(f_{n - 1, n} = h_{n,n}\).
From the first equation we find that
\[-h_{1,1} = f_{1, 2} = f_{n - 1, n} + \sum_{i = 1}^{n - 1} h_{ii} = \sum_{i = 1}^{n} h_{ii} \implies\]
\[\sum_i h_{ii} = 0.\]
Remark: this is just the statement that \(h \in \spl_n\); in other words, we will see that every choice of \(\spl_2\)-triple in \(\gl_{m + n}\) is also an \(\spl_2\)-triple in \(\spl_{m + n}\).
This shows that \(h_{11} = n - 1, h_{22} = n - 3, ..., h_{nn} = 1 - n\).
So we have determined \(h\); it is 
\[h = \begin{pmatrix}
    n - 1  & \\
    & n - 3 \\
    & & \ddots \\
    & & & 3 - n \\
    & & & & 1 - n 
\end{pmatrix}.\]
Now, we can use our expression for \(f\) in terms of \(h\) to obtain
\[f = \begin{pmatrix}
    0 & 1 - n \\
    & 0 & (1 - n) + (3 - n) \\
    & & \ddots & \ddots \\
    & & & 0 & (1 - n) + \cdots + (n - 1) \\
    & & & & 0
\end{pmatrix} = \]
\[\begin{pmatrix}
    0 & 1 (1 - n) \\
      & 0 & 2 (2 - n) \\
    & & 0 & (n - 2) (-2) \\
    & & & 0 & (n - 1) (-1)\\
    & &  & & 0
\end{pmatrix}.\]
\end{proof}

\begin{lemma}\label{sl2_triple}
    There is exactly one way to choose \(H, F \in \gl_{m + n}\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
\end{lemma}
\begin{proof}
Suppose we have \(H, F\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
Writing \(F =: \begin{pmatrix}
    F_{11} & F_{12} \\
    F_{21} & f
\end{pmatrix}\), and similarly for \(H\), we have
\[2E = [H, E] = \begin{pmatrix}
    0 & H_{12}e \\
    0 & he
\end{pmatrix} - \begin{pmatrix}
    0 & 0 \\
    e H_{21} & eh
\end{pmatrix},\]
\[H = [E, F] = \begin{pmatrix}
    0 & 0 \\
    e F_{21} & ef
\end{pmatrix} - \begin{pmatrix}
    0 & F_{12}e \\
    0 & fe
\end{pmatrix}.\]
From these we observe that \((e, h, f)\) must also be an \(\spl_2\)-triple, so \(h, f\) must be as in \cref{simple_sl2_triple}.
We also see that \(H_{11} = 0\).
Recalling that left multiplication by \(e\) is a down-shift, and right multiplication is a left-shift, we see that \(H_{12}\) is all zeroes except for the leftmost column, and \(H_{21}\) is all zeroes except for the bottom row.
% Then \(F_{12}\) must be all zeros except for the two leftmost columns, and \(F_{21}\) must be all zeroes except for the bottom two rows.
Then our final constraint is that 
\[-2F = [H, F] = \]
\[\begin{pmatrix}
    H_{12} F_{21} & H_{12} F_{22} \\
    H_{21} F_{11} + H_{22} F_{21} & H_{21} F_{12} + H_{22} F_{22}
\end{pmatrix} - \begin{pmatrix}
    F_{12} H_{21} & F_{11} H_{12} + F_{12} H_{22} \\
    F_{22} H_{21} & F_{21} H_{12} + F_{22} H_{22}
\end{pmatrix}.\]
Now \(H_{12} = F_{12}e\), and \(H_{21} = eF_{21}\), from the equation \(H = [E, F]\).
Substituting in the equation above then,
\[-2F = \begin{pmatrix}
    F_{12} e F_{21} & F_{12}e f \\
    eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    F_{12} e F_{21} & F_{11} F_{12}e + F_{12} h \\
    f eF_{21} & F_{21} F_{12}e + fh
\end{pmatrix} =\]
\[\begin{pmatrix}
    0 & F_{12}(fe + h) \\
    eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    0 & F_{11} F_{12}e + F_{12} h \\
    (ef - h)F_{21} & F_{21} F_{12}e + fh
\end{pmatrix} =\]
\[\begin{pmatrix}
    0 & F_{12}fe \\
    eF_{21} F_{11} & eF_{21} F_{12} + hf
\end{pmatrix} - \begin{pmatrix}
    0 & F_{11} F_{12}e \\
    ef F_{21} & F_{21} F_{12}e + fh
\end{pmatrix}.\]
Now we see that \(F_{11} = 0\), and consequently that \(F_{12} = F_{21} = 0\) as well.
This shows that \(H_{12} = H_{21} = 0\).
We conclude that \(H\) and \(F\) just have \(h\) and \(f\) in their bottom-right corners, respectively.
\end{proof}

\section{Finding the Slodowy slices with the same \(E\)}
First we find \(\ker \ad_f\).
We have \((fX)_{ij} = i (n - i) A_{i + 1,j}\), and \((Xf)_{ij} = (j - 1) (n - (j - 1)) A_{i, j - 1}\).
So, for all \(i, j \in \{1, ..., n\}\), we have 
\[i(n - i)A_{i + 1,j} = (j - 1)(n - (j - 1))A_{i, j - 1}.\]
Taking \(j = 1\), we find that \(A_{i,1} = 0\) for \(i \geq 2\).
Then, taking \(j > 1\), we find that for \(i, j \in \{1, ..., n - 1\}\),
\[A_{i + 1, j + 1} = \frac{(j - 1)(n - (j - 1))}{i(n - i)} A_{ij}.\]
So, \(\ker\ad_f\) is the set of matrices which are upper triangular and satisfy the above condition.
\par To find the Slodowy slice associated to the previous \(\spl_2\)-triple \((E, H, F)\), we just need to find \(\ker\ad_F\).
We have 
\[[F, X] = \begin{pmatrix}
    0 & 0 \\
    f X_{21} & fX_{22}
\end{pmatrix} - \begin{pmatrix}
    0 & X_{12} f\\
    0 & X_{22} f
\end{pmatrix}.\]
Thus, \(X_{21}\) must be all zeroes except for the first row, and \(X_{12}\) must be all zeroes except for the last column, and \(X_{22} \in \ker\ad_f\).
There is no restriction on \(X_{11}\).
This describes \(\ker\ad_F\).
\par For \(X \in \mathcal{S}_{m,n}\), define \(u(X) := X_{11}\).

\subsection{Finding \(\widetilde{\mathcal{N}}_{m,n}\)}
Let \(\mathcal{N}_m \subseteq \gl_m\) be the nilpotent elements.
Let \(\mathcal{S}_{m,n}'\) be the set of \(X \in \mathcal{S}_{m,n}\) such that both \(X\) and \(u(X)\) are nilpotent.
Let \(\widetilde{\mathcal{N}}_{m,n} = \{(\mathfrak{b}, X) : X \in \mathfrak{b}\} \subseteq \mathcal{B}_{m + n} \times \mathcal{S}_{m,n}'\).
Define \(\pi_{m,n} : \widetilde{\mathcal{N}}_{m,n} \to \mathcal{N}_m\) by \((\mathfrak{b}, X) \mapsto X_{11}\).
For \(Y \in \gl_m\), we call \(\pi_{m,n}^{-1}(Y)\) the \emph{Springer fiber at the \(n\)-Slodowy slice at \(Y\)}.

\begin{lemma}
    Let \(J\) be a jordan block with zeroes along the diagonal, and let \(A\) be upper triangular and nonzero.
    Then \(J + A\) is not nilpotent.
\end{lemma}
\begin{proof}
    It is straightforward to show by induction that if \(v_i = 0\) for \(i < j\), and \(v_j \neq 0\), then \(((J + A)^k v_j)_{j + k} = v_j\).
    Let \(i\) be such that \(Ae_i \neq 0\).
    Then \((J + A)^{i - 1} e_1\) has nonzero \(e_i\)-component.
    Then \((J + A)^i e_1\) has some nonzero \(e_{i'}\)-component for some \(i' \leq i\).
    Then \((J + A)^{i + (n - i')} e_1\) has some nonzero \(e_n\)-component.
    And \(i + (n - i') \geq n\), so we're done.
\end{proof}

\begin{lemma}
    Let \(X \in \gl_m\), and let 
    \[Y = \begin{pmatrix}
        y_{11} & y_{12} & y_{13} & \cdots & y_{1,n-1} & y_{1n} \\
        d_1 & y_{22} & y_{23} & \cdots & y_{2,n-1} & y_{2n} \\
            & d_2   & y_{33} & \cdots & y_{3,n-1} & y_{3n} \\
            & & \ddots & \cdots & \vdots  & \vdots \\
            & & & d_{n - 2} & y_{n-1,n-1} & y_{n - 1,n}\\
            & & & & d_{n - 1} & y_{nn}
    \end{pmatrix} \in \gl_n.\]
    For any \(a, b \in \mathbb{C}^m\),
    \[\det 
    \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray} = \]
       \[\det X \det Y + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
            & & & \vert \\
            & X & & b     \\
            & & & \vert \\
            \hline 
        \text{---} & a & \text{---} & 0
       \end{pNiceArray}\]
\end{lemma}
\begin{proof}
    By induction on \(n\).
    In the case \(n = 1\), expanding along the last row (taking the usual interpretation of the empty product) gives the desired result.
    \par Now suppose \(n > 1\).
    Expanding along the last row, we get 
    \[d_{n - 1} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n-1} & \\
       & & &  \\
       \end{pNiceArray} -
    y_{nn} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \\
        & X & & & &   \\
        & & & & & \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n} & \\
       & & &  \\
       \end{pNiceArray}.\]
    Using our inductive hypothesis for the first determiniant, and using that \(\det \begin{pNiceArray}{c|c}
        A_{11} & 0 \\
        A_{21} & A_{22}
    \end{pNiceArray} = \det A_{11} \det A_{22}\) for the second, the expression becomes
    \[d_{n - 1} \left(\det X \det Y_{n,n-1} + \left(\prod_{i \leq n - 2} d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}\right)
   - y_{nn} \det X \det Y_{nn} = \]
   \[(d_{n - 1} Y_{n,n-1} - y_{nn} \det Y_{nn})\det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray} = \]
   \[\det Y \det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}.\]
\end{proof}

\begin{corollary}
    If \(X\) is nilpotent, and
    \[\begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray}\]
    is nilpotent  as well, then \(Y\) is nilpotent (TODO: and that other determinant is zero).
    \begin{proof}
        By the previous lemma, the characteristic polynomial of the big matrix is
        \[g_X(\lambda) g_Y(\lambda) + f(\lambda),\]
        where \(g_X(\lambda) = \lambda^m\) is the characteristic polynomial of \(X\), and \(f(\lambda)\) is some polynomial of degree at most \(m - 1\).
    \end{proof}
\end{corollary}
\par Now, taking the previous corolloary and the first lemma together, we see that 
\[\mathcal{S}_{m,n}' = \left\{\begin{pNiceArray}{ccc|cccc}
    & & & & & & \vert \\
    & X & & & & & b    \\
    & & & & & & \vert \\
   \hline
   \text{---} & a & \text{---} & 0 \\
    & & & 1 & 0  & \\
    &  & & & \ddots & \ddots  \\
    & & & & & 1 & 0
   \end{pNiceArray} : a, b \in \mathbb{C}^m, X \in \gl_m \textrm{ is nilpotent}\right\}.\]



\section{Simplifying the definition of a Springer fiber at a Slodowy slice}
\par Let \(\mathcal{N}_m \subseteq \gl_m(\mathbb{C})\) be the subset consisting of nilpotent elements.
For \(X \in \mathcal{N}_m\) and \(a, b \in \mathbb{C}^m\), let
\[A_{X, a, b} = \begin{pNiceArray}{ccc|ccccc}
 & & & & & & & \vert \\
 & X & & & & & & b    \\
 & & & & & & & \vert \\
\hline
\text{---} & a & \text{---} & 0 \\
& & & 1 & 0 \\
& & &  & 1 & \ddots \\
& & &  &   & \ddots & 0 \\
& & &  &   &  & 1 & 0
\end{pNiceArray} \in \mathfrak{gl}_{m + n}(\mathbb{C}).\]
By the definition given in the previous section, we have 
\[\widetilde{\mathcal{N}}_{m, n} = \{(\mathfrak{b}, X) : X \in \mathfrak{b}\} \subseteq \mathcal{B}_{m + n} \times \mathcal{S}_{m,n}',\]
and the Springer fiber at the \(n\)-Slodowy slice at a nilpotent \(X \in \gl_m\) is
\[\pi_{m,n}^{-1}(X) = \{(\mathfrak{b}, A_{X, a, b}) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \in \mathcal{S}_{m, n}'\} \] 
\[= \{(\mathfrak{b}, A_{X, a, b}) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \textrm{nilpotent}\}\]
\[\cong \{(\mathfrak{b}, a, b) : A_{X, a, b} \in \mathfrak{b}, A_{X, a, b} \textrm{ nilpotent}\}.\]
\par Using our correspondence between \(\mathcal{B}_{m + n}\) and complete flags of \(\mathbb{C}^{m + n}\), we obtain 
\[\pi_{m,n}^{-1}(X) \cong \{(V, a, b) : \forall i. \; A_{X, a, b} V_i \subseteq V_{i - 1}\}.\]

\section{Strategy and setup for finding the irreducible components of a Springer fiber at a Slodowy slice}
\par Fix any \(X \in \mathcal{N}\).
As we have fixed \(X\), we now write \(A_{a, b} := A_{X, a, b}\).
Let \((e_{ij})_{1 \leq i \leq k, 1 \leq j \leq \lambda_i}\) be a Jordan basis for \(X\).
For convenience we define \(e_{i0} := 0\); now we may express the fact that \((e_{ij})\) is a Jordan basis by writing \(\forall i. Xe_{ij} = e_{i,j - 1}\).
\par In this section we find the irreducible components of
\[\pi^{-1}(X) \cong V := \{(A_{a, b}, U) : \forall i. \; A_{a, b} U_{i + 1} \subseteq U_i\}.\]
\par For \(1 \leq w \leq k\) and \(0 \leq r \leq \lambda_w\) (note that we allow \(r = 0\)), define 
\[V_{w,r} := \{(A_{a, b}, U) \in V : \exists P \in \GL_{m}. \; \exists b'. \; (P^{-1}, I_n) A_{a, b} (P, I_n) = A_{e_{wr}, b'} \}.\]

\begin{lemma}
    \(V = \bigcup_{1 \leq w \leq k, 0 \leq r \leq \lambda_{w}} V_{w,r}\).
    Further, \(V_{w_1r_1} = V_{w_2r_2}\) exactly when either \(r_1 = r_2 = 0\), or \(\lambda_{w_1} = \lambda_{w_2} \land r_1 = r_2\).
    When \(V_{w_1r_1} \neq V_{w_2r_2}\), we have \(V_{w_1r_1} \cap V_{w_2r_2} = \emptyset\).
\end{lemma}
\begin{proof}
    TODO
\end{proof}
Now, fix any \(w\) and \(r\).
We will find the irreducible components of \(V_{w,r}\).
These will all happen to be equidimensional (with dimensions independent of \(w\) and \(r\)), so their closures in \(V\) will be the irreducible components of \(V\).
\par Let 
\[G := \{P \in \GL_m : P^{-1} X P = X\},\]
and
\[G_{wr} = \{A \in G : e_{wr}A = e_{wr}\}.\]
\par Now, define 
\[U_{wr} = \{(A_{e_{wr}, b}, U) \in V_{wr}\}.\]
Let \(G\) act on \(V_{wr}\) by
\[P \cdot (A_{e_{wr},b}, U) := ((P, I_n) A_{e_{wr}, b} (P, I_n)^{-1}, (P, I_n)U) = (A_{e_{wr} P^{-1}, Pb}, (P, I_n) U).\]
\par Consider the map \(\varphi : U_{wr} \times G \to V_{wr}\) defined by
\[(x, P) \mapsto P \cdot x.\]
By restriction of \(G\) to \(G_{wr}\) and \(V_{wr}\) to \(U_{wr}\), we obtain an action of \(G_{wr}\) on \(U_{wr}\).
Then, letting \(G_{wr}\) act on \(G\) by \(g \cdot h := hg^{-1}\), we obtain an action of \(G_{wr}\) on \(U_{wr} \times G\).
\begin{lemma}
    As an algebraic variety, \(G\) is irreducible.
\end{lemma}
\begin{proof}
    It's just \(\mathbb{C}^{something}\), since its blocks are the lower-left-toeplitz matrices.
\end{proof}
\begin{lemma}
    The map \(\varphi\) is a principal \(G_{wr}\)-bundle.
\end{lemma}
\begin{proof}
    We need to show that \(G_{wr}\) acts freely and transitively on the fibers of \(\varphi\).
    It is obvious that \(G_{wr}\) acts freely on \(U_{wr} \times G\); it is enough to note that it acts freely on \(G\).
    \par Let \(y \in V_{wr}\).
    By definition of \(V_{wr}\), there is \(P_y \in G\) with \(P_y \cdot y \in U_{wr}\).
    We have
    \begin{align*}
        \varphi^{-1}(y) = \{(x, P) : P \cdot x = y \} & = \\
        \{(P^{-1} y, P) : P^{-1} y \in U_{wr} \} & = \\
        \{((P^{-1}P_y^{-1}) \cdot (P_y \cdot y), P) : (P^{-1} P_y^{-1}) \cdot (P_y \cdot y) \in U_{wr}\} & .
    \end{align*}
    For any \(x \in U_{wr}\) (in particular for \(x = P_y \cdot y\)), we have \(G_{wr} = \{g \in G : g \cdot x \in U_{wr}\}\).
    Therefore, the fiber above becomes
    \[\{((P^{-1}P_y^{-1}) \cdot (P_y \cdot y), P) : (P^{-1} P_y^{-1}) \in G_{wr}\}.\]
    Setting \(Q := P^{-1} P_y^{-1}\), so that \(P = P_y^{-1} Q^{-1}\), the above becomes
    \begin{align*}
        \{(Q \cdot (P_y \cdot y), P_y^{-1} Q^{-1}) : Q \in G_{wr}\} & = \\
        \{Q \cdot (P_y \cdot y, P_y^{-1}) : Q \in G_{wr}\} & .
    \end{align*}
        We see that the fibers are exactly the \(G_{wr}\)-orbits; or in other words, \(G_{wr}\) acts transitively on the fibers, as desired.
    % Because \(X\) and \(G\) are irreducible, \(X \times G\) is irreducible, and therefore \(\varphi(X \times G)\) is, as well.
    % Let \(X_1, ..., X_k\) be the irreducible components of \(U_{wr}\).
    % It is clear that \(V_{mr} = \bigcup_i \overline{\varphi(X_i \times G)}\).
    % And if we take \(x \in X_i \setminus \bigcup_{j \neq i} X_j\), it is clear (since \(G\) acts faithfully on itself) that \(\varphi(x, I_m)\)
    % All that remains is to show that \(X_i \notin \bigcup_{j \neq i} \overline{\varphi(X_i \times G)}\).
    % Indeed, 
\end{proof}
Our strategy is to find the irreducible components \(X \subseteq U_{wr}\), and we will then argue that the irreducible components of \(V_{wr}\) are of the form \(\varphi(X \times G)\).
So, we will now find the irreducible components of \(U_{wr}\).
\par Actually this will be unnecessarily difficult to think about; it is easiest in the case \(r = 0\).
So, we will change basis to make \(r = 0\).
Let \(\lambda' = (\lambda_1, ..., \lambda_{w - 1}, \lambda_w - r, \lambda_{w + 1}, ..., \lambda_k)\).
Let \(X'\) be in Jordan normal form with shape \(\lambda'\).
Let \(U_{wr}' = \{(A_{X', 0, b}, U) : \forall i. A_{X', 0, b} U_{i + 1} \subseteq U_i\}\).
Let \(e_{ij}'\) be a Jordan basis for \(X'\).
Let \(f_1, ..., f_n\) be a Jordan basis for the restriction of \(A_{X, 0, b}\) to \(\mathbb{}\)...
Set \(m' := m - r\), and \(n' := n + r\).
Let \(f_1', ..., f_{n'}'\) blah.
Define the linear map \(Q_{wr} : \mathbb{C}^{m' + n'} \to \mathbb{C}^{m + n}\) by:
\begin{itemize}
    \item For all \(i\), \(e_{ij}' \mapsto e_{ij}\).
    \item For \(j = 1, ..., r\), \(f_{n + j}' \mapsto e_{w,(\lambda_w - r) + j}\).
    \item For \(j = 1, ..., n\), \(f_j' \mapsto f_j + e_{w, (\lambda_w - r) - n + j}\).
\end{itemize}
% The inverse map \(Q_{wr}^{-1} : \mathbb{C}^{m + n} \to \mathbb{C}^{m' + n'}\) is
% \begin{itemize}
%     \item For all \(i \neq w\), \(e_{ij} \mapsto e_{ij}'\).
%     \item For \(j = 1, ..., \lambda_w - r\), \(e_{wj} \mapsto e_{wj}'\).
%     \item For \(j = 1, ..., r\), \(e_{wj} \mapsto f'_{n + j}\).
%     \item For \(j = 1, ..., n\), \(f_j \mapsto f'_j - e_{w,\lambda_w - n + r}\)
% \end{itemize}
Observe that conjugation by \(Q_{wr}\) maps \(U_{wr}\) to \(U'_{wr}\), and conjugation by \(Q_{wr}^{-1}\) maps \(U'_{wr}\) to \(U_{wr}\).
We conclude that \(U_{wr} \cong U_{wr}'\); so to find the irreducible components of \(U_{wr}\) we just need to find the irreducible components of \(U_{wr}'\).
To clear the context, which is rather cluttered by now, and to avoid writing primes everywhere, we move to a new section.

\section{Finding the irreducible components of \(U_0\)}
\subsection{Introduction}
Let \(X\) be nilpotent with Jordan basis \((e_{ij}')_{1 \leq i \leq k, 1 \leq j \leq \lambda_i'}\).
Let \(V = \{(A_{a,b}, U) : \forall i. \; A_{a,b} U_{i + 1} \subseteq U\}\).
Let
\[U_0 = \{(A_{0,b}, U) \in V\}.\]
We will find the irreducible components of \(U_0\).
\par We write \(b_{ij}\) to denote the projection of \(b \in \mathbb{C}^m\) onto \(e_{ij}\).
For each row \(i\), let \(p_i(b) = \max\{j : b_{ij} \neq 0\}\) (the maximum of the empty set is zero).
Then set \(q_i(b) = \lambda_i - p_i(b)\).
When it is clear enough from context, we will just write \(p_i\) and \(q_i\) instead of \(p_i(b)\) and \(q_i(b)\).
\par For any \(I \subseteq \{1, ..., k\}\) and any values \((\rho_i)_{i \in I}\) %such that (1) \(\rho_i \leq \lambda_i\), (2) \(\rho_i\) is decreasing with \(i\), and (3) \(\lambda_i - \rho_i\) is decreasing with \(i\)
, define 
\[B_{I, (\rho_i)} := \{b \in \mathbb{C}^m : \]
\[\{(p_i, q_i) : i \in I\} = \{(p_i, q_i) : n > q_i \land p_i = \max_{j : q_j = q_i} p_j > \max_{j : q_j < q_i} p_j \} \] 
\[\land \forall i \in I. \; p_i = \rho_i \}.\]
Then let \(U_{I, (\rho_i)} = \{(A_{0, b}, U) \in V : b \in B_{I, (\rho_i)}\}\).
\begin{lemma}\label{bs_union}
    \(\mathbb{C}^m = \bigcup_{I, (\rho_i)} B_{I, (\rho_i)}\), where \(I\) ranges over all subsets of \(\{1, ..., k\}\), and \((\rho_i)\) ranges over all maps \(I \to \mathbb{N}_{>0}\) such that \(\rho_i\) and \(\lambda_i - \rho_i\) are both decreasing with \(i\).
\end{lemma}
\begin{proof}
    Let \(b \in \mathbb{C}^m\).
    Let \(I' = \{(p_i, q_i) : p_i = n > q_i \land \max_{j : q_j = q_i} p_j > \max_{j : q_j < q_i} p_j\}\).
    Then let \(I = \{\max\{i : (p_i, q_i) = (p, q)\} : (p, q) \in I'\}\).
    Choosing the maximal \(i\) is an arbitrary choice; we just have to pick one.
    For \(i \in I\), \(\rho_i := p_i\).
    All we need to do is show that each \(\rho_i\) is positive, and that \(\rho_i\) and \(\lambda_i - \rho_i\) are decreasing.
    Then we get that \((A_{0,b}, U) \in U_{I, (\rho_i)}\), and we are done.
    \par Recall that by convention the maximum of the empty set is zero.
    So, the fact that \(p_i > \max_{j > i} p_j\) tells us that \(\rho_i = p_i\) is positive.
    \par Let \(i, j \in I\) with \(i < j\).
    Since \(\lambda_i \geq \lambda_j\), we have either \(p_i \geq p_j\) or \(q_i \geq q_j\).
    Further, since \((p_i, q_i) \neq (p_j, q_j)\) (clear from definition of \(I\)), either \(p_i > p_j\) or \(q_i > q_j\).
    And for \(i, j \in I\) we clearly have \(p_i > p_j \iff q_i > q_j\).
    So in either case, both \(p_i > p_j\) and \(q_i > q_j\), and consequently both \(\rho_i > \rho_j\) and \(\lambda_i - \rho_i > \lambda_j - \rho_j\).
\end{proof}

\begin{lemma}
    There are blah \(B_{I, (\rho_i)}\) with \((\rho_i)\) satisfying the constraints of \cref{bs_union}, and none is contained in the union of the others.
\end{lemma}
\begin{proof}
    seems hard to count.
    total number is product over L of (number of rows of length L) times (count where we assume there is only one row of each length).
    
\end{proof}

\begin{lemma}\label{alternative_bs}
    Here we have an alternative characterization of \(B_{I, (\rho_i)}\).
    Namely,
    \[B_{I, (\rho_i)} = \{b \in \mathbb{C}^m : \forall i \in I. \; p_i = \rho_i \land \forall i \notin I. \; p_i = \min(p_{i_k}, \lambda_i - q_{i_k}), \]
    \[\textrm{ where } k = ...\}\]
\end{lemma}
\begin{proof}
    TODO
\end{proof}

\begin{corollary}\label{bs_iso}
    \[B_{I, (\rho_i)} \cong \prod_{i \in I} \mathbb{C}^{blah} \times \prod_{i \notin I}\]
\end{corollary}

\subsection{Irreducible components of \(U_{I, (\rho_i)}\)}
Fix any \(I\) and \((\rho_i)\) satisfying the conditions of Lemma 8.1.
We will find the irreducible components of \(U_{I, (\rho_i)}\), and show that their closures are in fact irreducible components of \(U_0\).
\par I claim that \(A_{0,b}\) has the same shape for every \((A_{0, b}, U) \in U_{I, (\rho_i)}\).
Call this shape \(\mu\).
By finding a algebraic map taking \(b\) to a Jordan basis for \(A_{0,b}\), we will put \(U_{I, (\rho_i)}\) in isomorphism with the product (choice of \(b\)) \(\times\) (Springer fiber at \(J(\mu)\)).
Then we will use our result about the usual Springer fiber at \(J(\mu)\) to find the irreducible components of \(U_{I, (\rho_i)}\).
\par So, now we find a Jordan basis for \(A_{0, b}\).
\begin{lemma}
    Let \(P = \max_{i \in I} \rho_i\), and for \(i \in I\), let \(P_i = \max_{j \in I : j > i} \rho_j\).
    The following vectors give a Jordan basis for \(A_{0,b}\).
    (For convenience, we write \(A := A_{0,b}\) in this lemma and proof.)
    \begin{itemize}
        \item For \(i \notin I\), the chain of length \(p_i + q_i\) beginning with \(e_{i, p_i + q_i}\)
        \item The chain of length \(n + P\) beginning with \(f_n - (A^{n + P}f_n \rightshift n + P)\)
        \item For \(i \in I\), the chain of length \(q_i + P_i\) beginning with \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i)\), where \(v_i := A^{n - q_i} f_n - \sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i}\)
    \end{itemize}
\end{lemma}
\begin{proof}
    There are three things to check: (1) the chains are no longer than their claimed lengths, (2) the claimed lengths sum to \(m + n\), and (3) the span of the chains is \(\mathbb{C}^{m + n}\).
    \par We have defined \(P\) and \(P_i\) in terms of the \(\rho_i\) to make it clear that they only depend on \(I\) and the \(\rho_i\).
    Now we provide more useful characterizations.
    It is immediate from the definition of \(U_{I, (\rho_i)}\) that \(P = \max_{l : q_l < n} p_l\), and \(P_i = \max_{l : q_l < q_i} p_l\).
    \begin{proof}[Proof of (1)]
        It is obvious that a chain beginning with \(e_{i, p_i + q_i}\) has length \(p_i + q_i\).
        \par Now consider the chain beginning with \(f_n - (A^{n + P} f_n \rightshift n + P)\).
        Note that \(A^n f_n = b\), so \(A^{n + P} f_n = A^P b = b \leftshift P\).
        By shifting \(b\) left \(P\) times, we zero out all the rows \(i\) where \(q_i < n\).
        This ensures that the operation of shifting \(b \leftshift P\) right \(n + P\) times is invertible by shifting left \(n + P\) times.
        That is,
        \[A^{n + P} f_n = b \leftshift P = ((b \leftshift P) \rightshift n + P) \leftshift n + P = A^{n + P}(A^{n + P}f_n \rightshift n + P).\]
        This shows that the chain has length at most \(n + P\), as desired.
        \par Now, let \(i \in I\).
        We have \(v_i = A^{n - q_i} f_n - \sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i}\).
        First we note that \(q_i < n\) by the definition of \(U_{I, (\rho_i)}\), so the definition of \(v_i\) makes sense.
        We consider the chain beginning with \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i)\).
        Note that \(A^{q_i} v_i\) is just \(b\) with row \(i\) zeroed out.
        For brevity, we write \(b_i := A^{q_i} v_i\).
        Note that \(b_i \leftshift P_i\) has rows \(l\) zeroed out, for all \(l\) with \(q_l < q_i\).
        This ensures that shifting \(b_i \leftshift P_i\) right \(q_i + P_i\) times can be inverted by shifting left \(q_i + P_i\) times.
        That is,
        \[A^{q_i + P_i} v_i = b_i \leftshift P_i = ((b_i \leftshift P_i) \rightshift q_i + P_i) \leftshift q_i + P_i = A^{q_i + P_i}(A^{q_i + P_i} v_i \rightshift q_i + P_i).\]
        This shows that the chain has length at most \(q_i + P_i\), as desired.
    \end{proof}
    \begin{proof}[Proof of (2)]
        The sum of the lengths is
        \[\sum_{i \notin I} (p_i + q_i) + (n + P) + \sum_{i \in I} (q_i + P_i).\]
        Writing \(I = \{i_1 < \cdots < i_{|I|}\}\), we note that \(P = p_{i_1}\), that \(P_{i_{|I|}} = 0\), and that for \(l < |I|\) we have \(P_{i_l} = p_{i_{l + 1}}\).
        Thus, the sum above is 
        \[\sum_{i \notin I} (p_i + q_i) + (n + p_{i_1}) + \sum_{l = 1}^{|I|} (q_{i_l} + p_{i_{l + 1}}) = \]
        \[\sum_{i \notin I} (p_i + q_i) + n + \sum_{l = 1}^{|I|} (q_{i_l} + p_{i_l}) = n + \sum_i (q_i + p_i) = m + n.\]
    \end{proof}
    \begin{proof}[Proof of (3)]
        Let \(W\) be the span of the chains listed.
        We need to show that \(W = \mathbb{C}^{m + n}\).
        Because every \(i \in I\) satisfies \(q_i < n\), clearly \(\langle e_{ij}\rangle_{i,j : q_i \geq n} \subseteq W\).
        \par I claim that \(f_n \in W\) as well.
        To see this, we consider the chain beginning with \(f_n - (A^{n + P} f_n \rightshift n + P)\).
        As explained in the proof of (1), we have \(A^{n + P} f_n \in \langle e_{ij}\rangle_{i,j : q_i \geq n}\).
        Consequently, \((A^{n + P} f_n \rightshift n + P) \in \langle e_{ij} \rangle_{i,j : q_i \geq n} \subseteq W\).
        Because \(f_n - (A^{n + P} f_n \rightshift n + P) \in W\), this implies that \(f_n \in W\).
        \par Because \(AW \subseteq W\) (obvious, since \(W\) is the span of chains), the fact that \(f_n \in W\) implies that \(f_i \in W\) for each \(i\), and also \(b \leftshift l \in W\) for each \(l \geq 0\).
        \par Now we are left with showing that \(\langle e_{ij} \rangle_{i,j : q_i < n} \subseteq W\).
        This is obvious for \(i \notin I\).
        For \(i \in I\), we do it inductively.
        Fix \(i \in I\), and suppose we have already shown that for all \(i' \in I\) with \(q_{i'} > q_i\), we have \(\langle e_{ij} \rangle_{j \textrm{ arbitrary}} \subseteq W\).
        We will show that \(\langle e_{ij} \rangle_{j \textrm{ arbitrary}} \subseteq W\).
        \par To see this, we consider the chain beginning with \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i)\).
        (Recall \(v_i = A^{n - q_i} f_n - \sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i}\).)
        Because \(AW \subseteq W\), and \(b_{i,p_i} \neq 0\) (by definition of \(p_i\)), it suffices to show that \(\sum_{l = 1}^{p_i} b_{il} e_{i,k + q_i} \in W\).
        As explained in the proof of (1), we have \(A^{q_i + P_i} v_i \in \langle e_{lj} \rangle_{q_l \geq q_i}\).
        And since \(A^{q_i + P_i} v_i\) has row \(i\) zeroed out, in fact \(A^{q_i + P_i} v_i \in \langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i}\).
        Hence, \(A^{q_i + P_i} v_i \rightshift q_i + P_i \in \langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i}\).
        By our inductive hypothesis, \(\langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i} \subseteq W\), and consequently \(A^{q_i + P_i} v_i \rightshift q_i + P_i \in W\).
        Since we know \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i) \in W\), this implies that \(v_i \in W\).
        Because \(A^{n - q_i} f_n \in W\), this then implies that \(\sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i} \in W\), as desired.
    \end{proof}
    We proved (1), (2), (3), so we are done.
\end{proof}

Let \(\mu\) be the shape of the Jordan basis given in the previous lemma.
Let \(X_\mu\) be the Springer fiber at \(J(\mu)\).

\begin{lemma}
    \(U_{I, (\rho_i)} \cong B_{I, (\rho_i)} \times X_\mu\).
\end{lemma}
\begin{proof}
    For \(b \in B_{I, (\rho_i)}\), let \(P_b\) be the change-of-basis matrix, with columns given by the Jordan basis of the previous lemma, so that \(J(\mu) = P_b^{-1} A_{0,b} P_b\).
    From looking at the Jordan basis of the previous lemma (the basis can be expressed in terms of \(A, \lambda, e_{ij}, f_i, I, \rho_i, b\)), it is clear that the map \(P : B_{I, (\rho_i)} \to \GL_{m + n}\), given by \(b \mapsto P_b\), is algebraic.
    \par Now, we remark that the Springer fiber at \(A_{0,b}\) is simply \(\{P_b U : U \in X_\mu\}\).
    This gives us the isomorphism \(B_{I, (\rho_i)} \times X_\mu \to U_{I, (\rho_i)}\).
    \[(b, U) \mapsto (P_b J(\mu) P_b^{-1}, P_b U),\]
    with inverse
    \[(A_{0,b}, U) \mapsto (b, P_b^{-1} U).\]
\end{proof}
At last we have reached the base of our recursive procedure, and we begin to propagate upwards.
We write \((X_{\mu, \alpha})_\alpha\) to denote the irreducible components of \(X_\mu\).
\begin{lemma}
    The irreducible components of \(U_{I, (\rho_i)}\) are exactly the subvarieties 
    \[U_{I, (\rho_i), \alpha} := \{(P_b J(\mu) P_b^{-1}, P_b U) : b \in B_{I, (\rho_i)}, U \in X_{\mu, \alpha}\}.\]
    These are distinct.  
    There are blah of them, and each has dimension blah.
\end{lemma}
\begin{proof}
    We know from \cref{bs_iso} that \(B_{I, (\rho_i)}\) is irreducible.
    So, the components of \(B_{I, (\rho_i)} \times X_\mu\) are obviously \(B_{I, (\rho_i)} \times X_{\mu, \alpha}\).
    Their dimensions are blah, and there are... of them.
    taking image under the isomorphism gives the desired result.
\end{proof}

\subsection{Conclusion}
\begin{lemma}
    The irreducible components of \(U_0\) are exactly the \(U_{I, (\rho_i), \alpha}\).
    They are distinct.
    There are blah of them, each of dimension blah.
\end{lemma}

\section{Finding the irreducible components of a springer fiber at a slodowy slice}
Apply things from the previous two sections, and conclude.

\section{A different variety}
Define \(R = \{(X, \mathfrak{b}_1, \mathfrak{b}_2) : X \in \mathfrak{b}_1 \land u(X) \in \mathfrak{b}_2\} \subseteq \mathcal{S}_{m,n}' \times \mathcal{B}_{m + n} \times \mathcal{B}_m\).
We can obtain a subvariety of \(R\) by requiring that \(u(X)\) is in some fixed similarity class.
(TODO: why is this a subvariety?  Is it?  Is this even the right way of explaining the significance of the \(m^2\)?)
We expect that each of these subvarieties is an irreducible component of dimension \(m^2\).
We will verify these things using our prvious computations of springer fibers.

\subsection{TODO}
\begin{itemize}
    \item Why are SOn flags what they are.
\end{itemize}

\bibliography{paper}
\end{document}
 