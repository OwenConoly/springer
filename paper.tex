\documentclass[12pt,psamsfonts]{article}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{blkarray}
\usepackage{upquote}
\usepackage{tikz}
\usepackage{bbm}
\usepackage{url}
\usepackage{tikz-cd}
\usepackage{enumerate}
\usepackage{rotating}

\newcommand{\leftshift}{\,\texttt{<<}\,}
\newcommand{\downshift}{\mathbin{\rotatebox[origin=c]{90}{\leftshift}}}
\newcommand{\rightshift}{\mathbin{\rotatebox[origin=c]{180}{\leftshift}}}
\newcommand{\upshift}{\mathbin{\rotatebox[origin=c]{270}{\leftshift}}}

\newcommand{\deriv}[2]{\frac{\mathrm{d}#1}{\mathrm{d}#2}}
\newcommand{\pderiv}[2]{\frac{\partial #1}{\partial #2}}
\DeclareMathOperator{\spn}{span}
\DeclareMathOperator{\stab}{stab}
\DeclareMathOperator{\ord}{ord}
\DeclareMathOperator{\Ind}{Ind}
\DeclareMathOperator{\Res}{Res}
\DeclareMathOperator{\Image}{Image}
\DeclareMathOperator{\cof}{cof}
\DeclareMathOperator{\lcm}{lcm}
\DeclareMathOperator{\real}{Re}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\atan2}{atan2}
\DeclareMathOperator{\interior}{int}
\DeclareMathOperator{\Br}{Br}
\DeclareMathOperator{\End}{End}
\DeclareMathOperator{\Aut}{Aut}
\DeclareMathOperator{\Gal}{Gal}
\DeclareMathOperator{\inv}{inv}
\DeclareMathOperator{\GL}{GL}
\DeclareMathOperator{\SL}{SL}
\DeclareMathOperator{\gl}{\mathfrak{gl}}
\DeclareMathOperator{\spl}{\mathfrak{sl}}
\DeclareMathOperator{\ad}{ad}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\opp}{\mathrm{opp}}
\newcommand{\un}{\mathrm{un}}
\newcommand{\al}{\mathrm{al}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\comment}{}

\usepackage[capitalize,nameinlink,noabbrev]{cleveref} % to emulate \autoref style


\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{corollary}[theorem]{Corollary}
% \newcommand{\theoremautorefname}{Theorem}
\newcommand{\lemmaautorefname}{Lemma}
\newcommand{\definitionautorefname}{Definition}

\bibliographystyle{plain}
\usepackage{nicematrix}

% \author{Owen Conoly}
% \date{Date}
% \title{Title}

\begin{document}

% \maketitle

\section{Introduction}
Let \(\mathfrak{g} = \gl_m \times \gl_{m + n}\), with \(n > 0\).
% The nilpotent cone in \(\mathfrak{g}\) is \(\mathcal{N} = \mathcal{N}_m \times \mathcal{N}_{m + n}\), where \(\mathcal{N}_m\) and \(\mathcal{N}_{m + n}\) are the nilpotent cones in \(\gl_m\) and \(\gl_{m + n}\) respectively.
% Let \(\mathcal{B}_m\) and \(\mathcal{B}_{m + n}\) be the varieties of Borel subalgebras of \(\gl_m\) and \(\gl_n\) respectively.
% The variety of Borel subalgebras of \(\mathcal{N}\) is \(\mathcal{B} = \mathcal{B}_m \times \mathcal{B}_{m + n}\).
% Let
% \[\widetilde{\mathcal{N}} := \{(\mathfrak{b}, n) \in \mathcal{B} \times \mathcal{N} : n \in \mathfrak{b}\} = \]
% \[\{(\mathfrak{b}_m, \mathfrak{b}_{m + n}, n_m, n_{m + n}) \in \mathcal{B}_m \times \mathcal{B}_{m + n} \times \mathcal{N}_m \times \mathcal{N}_{m + n} : n_m \in \mathfrak{b}_m, n_{m + n} \in \mathfrak{b}_{m + n}\}.\]
Let \(\pi : \widetilde{\mathcal{N}} \to \mathcal{N} \hookrightarrow \mathfrak{g}\) be the Springer resolution.
\par Let \((e, h, f)\) be the principal \(\spl_2\)-triple in \(\gl_n\).
We have an embedding \(\gl_n \hookrightarrow \gl_m \times \gl_n \hookrightarrow \gl_{m + n}\).
Let \((E, H, F)\) be the \(\spl_2\)-triple in \(\gl_{m + n}\) which is the image of \((e, h, f)\) via this embedding.
Let \(S\) be the Slodowy slice \(F + \mathfrak{z}_{\gl_m}(E)\).
We have a map \(\gl_{m + n} \to \gl_m\) given by coordinate projection.
Restricting to \(S\), we obtain \(f_1 : S \to \gl_m\).
(It turns out that this map is surjective.)
Then, we obtain \(p_1 : S \to \mathfrak{g}\) by \(x \mapsto (x, f_1(x))\).
Taking the map \(p_1 : S \to \mathfrak{g}\) and the Springer resolution \(\widetilde{\mathcal{N}} \to \mathfrak{g}\), we obtain a fibred product \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
\par In this paper we study \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
% The first few sections will be finding what it looks like (in particular, what are the elements of \(S\) which project to nilpotent elements of \(\mathfrak{g}\)), and then we will find the irreducible components of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
As a step towards studying \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\), we consider the map \(S \times_\mathfrak{g} \widetilde{\mathcal{N}} \to \gl_m\) given by taking the map to \(\mathfrak{g}\) and then projecting to \(\gl_m\).
The main section of this paper will study the fibers of this map.
Given \(X \in \mathcal{N}_m \subseteq \gl_m\), we call the fiber at \(X\) the \emph{\(n\)-Slodowy-slice Springer fiber at \(X\)}.
\par In section 2, we review some preliminary material.
In section 3, we find the unique (up to similarity) principal \(\spl_2\)-triple in \(\gl_n\).
In section 4, we embed this \(\spl_2\)-triple into \(\gl_{m + n}\) as described above.
We compute the Slodowy slice, and we end up with a nice description of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
\par In section 5, we discuss how to reduce the problem of finding the irreducible components of an \(n\)-Slodowy-slice Springer fiber at \(X \in \mathcal{N}_m\) to an easier problem.
In section 6 we solve the easier problem.
In section 7 we find the irreducible components of an \(n\)-Slodowy-slice Springer fiber.
In section 8 we apply the results of section 7 to find the irreducible components of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
In section 9 we discuss an interesting direction for future work.
Then in section 10 we prove some linear algebra facts that were used in the paper.

\section{Preliminary Definitions and Facts}
\subsection{Conventions and Notations}
We write \(\GL_m, \spl_m\) to denote \(\GL_m(\mathbb{C}), \spl_m(\mathbb{C})\), and so on.
By \(J_m\) we refer to the nilpotent \(m \times m\) Jordan block (which, by convention, has ones \emph{below} the diagonal).
Given a partition \(\lambda = (\lambda_1, ..., \lambda_k)\), we write \(J(\lambda)\) to denote the block matrix
\[\begin{pmatrix}
    J_{\lambda_1} \\
    & \ddots \\
    & & J_{\lambda_k}
\end{pmatrix}.\]

\subsection{Springer fibers}
Let \(\mathfrak{g}\) be a Lie algebra.
Let \(\mathcal{N} \subseteq \mathfrak{g}\) be the subset consisting of nilpotent elements.
Let \(\mathcal{B}\) be the variety of Borel subalgebras of \(\mathfrak{g}\).
Let \(\widetilde{\mathcal{N}} = \{(\mathfrak{b}, n) : n \in \mathfrak{b}\} \subseteq \mathcal{B} \times \mathcal{N}\).
Let \(\pi : \widetilde{\mathcal{N}} \to \mathcal{N}\) be the projection onto the second coordinate.
We call this the \emph{Springer resolution}.
For \(n \in \mathcal{N}\), we call \(\pi^{-1}(n)\) the \emph{Springer fiber at \(n\)}.

\subsection{Springer fibers in \(\gl_m\)}
Now we let \(\mathcal{N}_m\) be the set of nilpotent elements in \(\gl_m\), and \(\mathcal{B}_m\) the variety of Borel subalgebras of \(\gl_m\).
Let \(H \subseteq \gl_m\) be the subalgebra of upper triangular matrices.
The variety of Borel subalgebras of \(\gl_m\) is \(\mathcal{B} = \{gHg^{-1} : g \in \GL_m\}\).
Thus, the Springer fiber at \(X \in \mathcal{N}\) is 
\[S_X = \{gHg^{-1} : X \in gHg^{-1}\}.\]
\begin{definition}
    A \emph{flag} \((V_i)\) of \(\mathbb{C}^m\) is a sequence of subspaces
    \[0 = V_0 \subseteq V_1 \subseteq \cdots \subseteq V_m = \mathbb{C}^m,\]
    where \(\dim V_i = i\).
\end{definition}
We say that \(X \in \gl_m\) \emph{preserves} a flag \((V_i)\) if \(\forall i. \; XV_i \subseteq V_i\).
Note that \(X \in \mathcal{N}\) preserves a flag \((V_i)\) if and only if \(\forall i. \; XV_i \subseteq V_{i - 1}\).
\par The simplest flag is the \emph{standard flag} \((E_i)\), where \(E_i := \langle e_1, ..., e_i \rangle\).
Note that the group \(H\) is exactly the subset of \(\gl_m\) which preserves \(E_i\).
\par We can think of \(\mathcal{B}\) as the set of flags of \(\mathbb{C}^m\), via the correspondence 
\[gHg^{-1} \leftrightarrow (gE_i).\]
Note that \(X\) preserves \((gE_i)\) if and only if \(X \in gHg^{-1}\).
Thus, we may write the Springer fiber at \(X \in \mathcal{N}\) in terms of flags, as 
\[S_X = \{(gE_i) : X \in gHg^{-1}\} = \{(V_i) : \forall i. \; XV_i \subseteq V_{i - 1}\}.\]

\begin{theorem}\label{usual_springer_fiber}
    (Needs citation!)
    The irreducible components of the Springer fiber at \(J(\mu)\) are in bijection with the standard Young tableaus of shape \(\mu\).
    Further, the irreducible components are equidimensional, of dimension \(\sum_{i \neq j} \min(\mu_i, \mu_j) = \sum_i (i - 1) \mu_i\).
\end{theorem}

\subsection{Slodowy Slices}

A basis for \(\spl_2\) is
\[e' := \begin{pmatrix}0 & 1 \\ 0 & 0 \\\end{pmatrix}, h' := \begin{pmatrix}1 & 0 \\0 & -1\end{pmatrix}, f' := \begin{pmatrix}0 & 0 \\1 & 0\end{pmatrix}.\]
Given a Lie algebra \(\mathfrak{g}\), and a homomorphism \(\phi : \spl_2 \to \mathfrak{g}\) sending \((e', h', f')\) to \((e, h, f)\), we say that \((e, h, f)\) is an \emph{\(\spl_2\)-triple}.
Observe that \(e, f\) must be nilpotent, and \(h\) must be Cartan (TODO: what?)
If \(\mathfrak{g}\) is semisimple, then given any nilpotent \(e \in \mathfrak{g}\), the Jacobson-Morozov theorem \cite[3.7.1]{ehf} says that there exist \(h, f \in \mathfrak{g}\) such that \((e, h, f)\) is an \(\spl_2\)-triple.
\par Given \((e, h, f)\), we define the \emph{Slodowy slice at \(e\)} as \(\mathcal{S}_e := e + \ker \ad_f\).
By the Jacobson-Morozov theorem, we can always find a Slodowy slice at any nilpotent \(e \in \mathfrak{g}\), when \(\mathfrak{g}\) is semisimple.

\section{Principal \(\spl_2\)-triple in \(\gl_n\)}
The unique nilpotent regular element of \(\gl_n\) (up to similarity) is \(J_n\).
In this section we use this to find that there is a unique principal \(\spl_2\)-triple in \(\gl_n\).
Write \(e = J_n\).
\begin{lemma}\label{simple_sl2_triple}
    There is exactly one way to choose \(h, f \in \gl_n\) so that \((e, h, f)\) is an \(\spl_2\)-triple.
\end{lemma}
\begin{proof}
    
Note that \([h', e'] = 2e'\), and \([e', f'] = h'\), and \([h', f'] = -2f'\).
Thus \(e, h, f\) must obey the same relations.
In particular, \(he - eh = 2e\).  
The matrix \(eh\) is \(h\) shifted down one, and \(he\) is \(h\) shifted left one.
Thus, \(2e_{ij} = (he - eh)_{ij} = h_{i, j + 1} - h_{i - 1, j}\).
We can use this to show that \(h_{ij} = 0\) when \(i \neq j\).
Then we can use it to show that \(h_{ii} = h_{i - 1, i - 1} + 2\), so that \(h_{ii} = h_{11} + 2(i - 1)\).
\par Similarly, from \([e, f] = h\) we get that \(h_{ij} = (ef - fe)_{ij} = f_{i - 1, j} - f_{i, j + 1}\).
We can use this to show that \(f_{ij} = 0\) when \(j \neq i + 1\).
Then we can use it to show that \(f_{i,i + 1} = f_{i + 1, i + 2} + h_{i + 1, i + 1}\), that \(f_{1,2} = -h_{1, 1}\), and that \(f_{n - 1, n} = h_{n,n}\).
From the first equation we find that
\[-h_{1,1} = f_{1, 2} = f_{n - 1, n} + \sum_{i = 1}^{n - 1} h_{ii} = \sum_{i = 1}^{n} h_{ii} \implies\]
\[\sum_i h_{ii} = 0.\]
Remark: this is just the statement that \(h \in \spl_n\); in other words, we will see that every choice of \(\spl_2\)-triple in \(\gl_{m + n}\) is also an \(\spl_2\)-triple in \(\spl_{m + n}\).
This shows that \(h_{11} = n - 1, h_{22} = n - 3, ..., h_{nn} = 1 - n\).
So we have determined \(h\); it is 
\[h = \begin{pmatrix}
    n - 1  & \\
    & n - 3 \\
    & & \ddots \\
    & & & 3 - n \\
    & & & & 1 - n 
\end{pmatrix}.\]
Now, we can use our expression for \(f\) in terms of \(h\) to obtain
\[f = \begin{pmatrix}
    0 & 1 - n \\
    & 0 & (1 - n) + (3 - n) \\
    & & \ddots & \ddots \\
    & & & 0 & (1 - n) + \cdots + (n - 1) \\
    & & & & 0
\end{pmatrix} = \]
\[\begin{pmatrix}
    0 & 1 (1 - n) \\
      & 0 & 2 (2 - n) \\
    & & 0 & (n - 2) (-2) \\
    & & & 0 & (n - 1) (-1)\\
    & &  & & 0
\end{pmatrix}.\]
\end{proof}

% \section{Finding \(\spl_2\)-triples \((E, H, F)\) in \(\gl_{m + n}\) with a particular \(E\).}
% TODO: I am unsure why this section is interesting or what to say about it.
% Should I delete it?
% For now I will ignore it and try to make the paper flow nicely without it.

% Let \(E = \begin{pmatrix}0 & 0 \\ 0 & e\end{pmatrix} \in \gl_{m + n}\).
% We will show that there is exactly one \(\spl_2\)-triple \((E, H, F)\), and we will find what it looks like.
% % First we solve the case \(m = 0\) (so \(E = e\)), and then we use this to solve the case of arbitrary \(m\).

% \begin{lemma}\label{sl2_triple}
%     There is exactly one way to choose \(H, F \in \gl_{m + n}\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
% \end{lemma}
% \begin{proof}
% Suppose we have \(H, F\) so that \((E, H, F)\) is an \(\spl_2\)-triple.
% Writing \(F =: \begin{pmatrix}
%     F_{11} & F_{12} \\
%     F_{21} & f
% \end{pmatrix}\), and similarly for \(H\), we have
% \[2E = [H, E] = \begin{pmatrix}
%     0 & H_{12}e \\
%     0 & he
% \end{pmatrix} - \begin{pmatrix}
%     0 & 0 \\
%     e H_{21} & eh
% \end{pmatrix},\]
% \[H = [E, F] = \begin{pmatrix}
%     0 & 0 \\
%     e F_{21} & ef
% \end{pmatrix} - \begin{pmatrix}
%     0 & F_{12}e \\
%     0 & fe
% \end{pmatrix}.\]
% From these we observe that \((e, h, f)\) must also be an \(\spl_2\)-triple, so \(h, f\) must be as in \cref{simple_sl2_triple}.
% We also see that \(H_{11} = 0\).
% Recalling that left multiplication by \(e\) is a down-shift, and right multiplication is a left-shift, we see that \(H_{12}\) is all zeroes except for the leftmost column, and \(H_{21}\) is all zeroes except for the bottom row.
% % Then \(F_{12}\) must be all zeros except for the two leftmost columns, and \(F_{21}\) must be all zeroes except for the bottom two rows.
% Then our final constraint is that 
% \[-2F = [H, F] = \]
% \[\begin{pmatrix}
%     H_{12} F_{21} & H_{12} F_{22} \\
%     H_{21} F_{11} + H_{22} F_{21} & H_{21} F_{12} + H_{22} F_{22}
% \end{pmatrix} - \begin{pmatrix}
%     F_{12} H_{21} & F_{11} H_{12} + F_{12} H_{22} \\
%     F_{22} H_{21} & F_{21} H_{12} + F_{22} H_{22}
% \end{pmatrix}.\]
% Now \(H_{12} = F_{12}e\), and \(H_{21} = eF_{21}\), from the equation \(H = [E, F]\).
% Substituting in the equation above then,
% \[-2F = \begin{pmatrix}
%     F_{12} e F_{21} & F_{12}e f \\
%     eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
% \end{pmatrix} - \begin{pmatrix}
%     F_{12} e F_{21} & F_{11} F_{12}e + F_{12} h \\
%     f eF_{21} & F_{21} F_{12}e + fh
% \end{pmatrix} =\]
% \[\begin{pmatrix}
%     0 & F_{12}(fe + h) \\
%     eF_{21} F_{11} + h F_{21} & eF_{21} F_{12} + hf
% \end{pmatrix} - \begin{pmatrix}
%     0 & F_{11} F_{12}e + F_{12} h \\
%     (ef - h)F_{21} & F_{21} F_{12}e + fh
% \end{pmatrix} =\]
% \[\begin{pmatrix}
%     0 & F_{12}fe \\
%     eF_{21} F_{11} & eF_{21} F_{12} + hf
% \end{pmatrix} - \begin{pmatrix}
%     0 & F_{11} F_{12}e \\
%     ef F_{21} & F_{21} F_{12}e + fh
% \end{pmatrix}.\]
% Now we see that \(F_{11} = 0\), and consequently that \(F_{12} = F_{21} = 0\) as well.
% This shows that \(H_{12} = H_{21} = 0\).
% We conclude that \(H\) and \(F\) just have \(h\) and \(f\) in their bottom-right corners, respectively.
% \end{proof}

\section{Finding \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\)}
\subsection{Finding the Slodowy slice \(S\)}
In the previous section we computed the principal \(\spl_2\)-triple \((e, h, f)\) in \(\gl_n\).
Embedding this into \(\gl_{m + n}\) as described previously, we obtain
\[(E, H, F) = \left(\begin{pmatrix}0 & 0 \\0 & e\end{pmatrix}, \begin{pmatrix}0 & 0 \\0 & h\end{pmatrix}, \begin{pmatrix}0 & 0 \\0 & f\end{pmatrix}\right).\]

\begin{lemma}\label{small_centralizer}
    \(\mathfrak{z}_{\gl_n}(f)\) is the set of upper-triangular \(X \in \gl_n\) such that for all \(i,j\),
    \[X_{i + 1,j + 1} = \frac{j(n - j)}{i(n - i)} X_{ij}.\]
\end{lemma}
\begin{proof}
    Let \(X \in \mathfrak{z}_{\gl_n}(f)\).
    Looking at the definition of \(f\) from the previous section (and zero-padding the matrices), we see that \((fX)_{ij} = i (n - i) X_{i + 1,j}\), and \((Xf)_{ij} = (j - 1)(n - (j - 1)) X_{i, j - 1}\).
    So,
    \[\forall i, j \in \{1, ..., n\}. \; i(n - i)X_{i + 1,j} = (j - 1)(n - (j - 1))X_{i, j - 1}.\]
    Taking \(j = 1\), the condition above tells us that \(\forall i \geq 2. \; X_{i, 1} = 0\).
    Taking \(j > 1\) and \(i < n\), we obtain that
    \[\forall i, j \in \{1, ..., n - 1\}. \; X_{i + 1, j + 1} = \frac{j(n - j)}{i(n - i)} X_{ij}.\]
    So, every \(X \in \mathfrak{z}_{\gl_n}(f)\) is upper triangular and satisfies the condition above.
    Conversely, it is clear that for such \(X\) we have \(fX = Xf\).
\end{proof}

\begin{lemma}\label{big_centralizer}
    \[\mathfrak{z}_{\gl_{m + n}}(F) =  \left\{\begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
        &  & &  & Y  \\
        & & & & & 
       \end{pNiceArray} : X \in \gl_m; Y \in \mathfrak{z}_{\gl_n}(f); a,b \in \mathbb{C}^m \right\}.\]
\end{lemma}
\begin{proof}
    Let \(Z = \begin{pmatrix}Z_{11} & Z_{12}\\Z_{21} & Z_{22}\end{pmatrix} \in \mathfrak{z}_{m + n}(F)\).
    We have 
    \[\begin{pmatrix}0 & Z_{12}f\\0 & Z_{22} f\end{pmatrix} =  ZF = FZ = \begin{pmatrix}0 & 0\\fZ_{21} & fZ_{22}\end{pmatrix}.\]
    We see that there is no restriction on \(Z_{11}\).
    The condition \(Z_{12}f = 0\) means that all but the last column of \(Z_{12}\) must be zero, and the condition \(0 = fZ_{21}\) means that all but the first row of \(Z_{21}\) must be zero.
    And the condition \(Z_{22}f = fZ_{22}\) means that \(Z_{22} \in \mathfrak{z}_{\gl_{m + n}}(f)\).
\end{proof}

These lemmas provide an explicit characterization of the Slodowy slice \(S = E + \mathfrak{z}_{\gl_{m + n}}(F)\).
\begin{corollary}\label{slodowy_slice}
    \[S = \left\{\begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
        & & &  & e + Y\\
        &  & &  &   
       \end{pNiceArray} : a, b \in \mathbb{C}^m, X \in \gl_m, Y \in \mathfrak{z}_{\gl_n}(f)\right\}.\]
\end{corollary}

\subsection{A description of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\)}
Recall that we have the map \(S \to \mathfrak{g}\) given by \(Z \mapsto (p(Z), Z)\), where \(p : \gl_{m + n} \to \gl_m\) is the coordinate projection.
We also have the Springer resolution \(\widetilde{\mathcal{N}} \to \mathfrak{g}\).
And from these two maps we define the fibered product \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).
\par To obtain an explicit description of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\), we will begin by finding the image of the projection \(S \times_\mathfrak{g} \widetilde{\mathcal{N}} \to S\).
As the image of the Springer resolution is \(\mathcal{N} = \mathcal{N}_m \times \mathcal{N}_{m + n}\), the image of the projection \(S \times_\mathfrak{g} \widetilde{\mathcal{N}} \to S\) is simply \(S' = \{Z \in S : p(Z) \in \mathcal{N}_m, Z \in \mathcal{N}_{m + n}\}\).
\begin{lemma}
    \[S' = \left\{A_{X, a, b} := \begin{pNiceArray}{ccc|cccc}
    & & & & & & \vert \\
    & X & & & & & b    \\
    & & & & & & \vert \\
   \hline
   \text{---} & a & \text{---} & 0 \\
    & & & 1 & 0  & \\
    &  & & & \ddots & \ddots  \\
    & & & & & 1 & 0
   \end{pNiceArray} \in \mathcal{N}_{m + n}: a, b \in \mathbb{C}^m, X \in \mathcal{N}_m\right\}.\]
\end{lemma}
\begin{proof}
    As every element of \(\mathfrak{z}_{\gl_n}(f)\) is upper triangular, \cref{bottom_right_nilp} tells us that if a matrix of the form given in \cref{slodowy_slice} is nilpotent, and the upper-left block \(X\) is nilpotent as well, then \(e + Y\) must be nilpotent.
    Then, again using that every element \(Y \in \mathfrak{z}_{\gl_n}(f)\) is upper triangular, \cref{upper_triangle_zero} tells us that if \(e + Y\) is nilpotent, then \(Y = 0\).
    \par Hence every element of \(S'\) must simply have \(e\) in its bottom-right block.
    We see that every element of \(S'\) is of the desired form.
\end{proof}
This is not a fully explicit characterization of \(S'\), since we don't say which choices of \(X\) and \(a, b \in \mathbb{C}^m\) lead to \(A_{X, a, b}\) being nilpotent.
We could use \cref{block_determinant} to find a necessary and sufficient condition on \(X, a, b\); however, the description above of \(S'\) will be good enough for our purposes.
\begin{corollary}
    \begin{align*}
        S \times_\mathfrak{g} \widetilde{\mathcal{N}} & = \\
        \{((X, A_{X, a, b}), ((X, A_{X, a, b}), \mathfrak{b})) : \mathfrak{b} \in \mathcal{B}, (X, A_{X, a, b}) \in \mathcal{N} \cap \mathfrak{b}\} & \cong \\
        \{(A_{X, a, b}, \mathfrak{b}) : \mathfrak{b} \in \mathcal{B}, (X, A_{X, a, b}) \in \mathfrak{b} \cap \mathcal{N}\} & .
    \end{align*}
\end{corollary}
We have a map \(\pi : S \times_\mathfrak{g} \widetilde{\mathcal{N}} \to \mathcal{N}_m\) given by \((A_{X, a, b}, \mathfrak{b}) \mapsto X\).
We define the \emph{\(n\)-Slodowy-slice Springer fiber} at \(X \in \mathcal{N}_m\) to be the fiber \(\pi^{-1}(X)\).
Because \(\mathcal{B} = \mathcal{B}_m \times \mathcal{B}_{m + n}\),  
\[\pi^{-1}(X) \cong \{(A_{X, a, b}, \mathfrak{b}_{m + n}) : A_{X, a, b} \in \mathfrak{b}_{m + n} \cap \mathcal{N}_{m + n}\} \times \{\mathfrak{b}_m : X \in \mathfrak{b}_m\}.\]
The right factor of the product is simply the usual Springer fiber at \(X\).
\par Let 
\[\mathcal{P}_X = \{(A_{X, a, b}, (V_i)) : \forall i. \; A_{X, a, b} V_i \subseteq V_{i - 1}\}.\]
Using our correspondence between Springer fibers in \(\mathcal{B}_{m + n}\) and flags of \(\mathbb{C}^{m + n}\), we see that \(\mathcal{P}_X\) is isomorphic to the left factor of \(\pi^{-1}(X)\).
In the next few sections, we will find the irreducible components of \(\mathcal{P}_X\).
Then we will use this, along with the result about the usual Springer fiber at \(X\), to find the irreducible components of \(\pi^{-1}(X)\) and then of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\).

\section{Strategy for finding components of \(\mathcal{P}_{J(\lambda)}\)}
\par Let \(X = J(\lambda) \in \gl_m\), where \(\lambda = (\lambda_1, ..., \lambda_k)\).
In this section we write \(\mathcal{P} := \mathcal{P}_{J(\lambda)}\) and \(A_{a, b} := A_{J(\lambda), a, b}\).
Let \((e_{ij})_{1 \leq i \leq k, 1 \leq j \leq \lambda_i}\) be the standard basis for \(\mathbb{C}^m\).
For convenience we write \(e_{ij} := 0\) for \(j < 1\); now we may express the fact that \((e_{ij})\) is a Jordan basis by writing \(\forall i. Xe_{ij} = e_{i,j - 1}\).
\par In this section we begin finding the irreducible components of
\[\mathcal{P} = \{(A_{a, b}, (V_i)) : \forall i. \; A_{a, b} V_{i + 1} \subseteq V_i\}.\]
\par For \(1 \leq w \leq k\) and \(0 \leq r \leq \lambda_w\) (note that we allow \(r = 0\)), define 
\[\mathcal{P}_{w,r} := \{(A_{a, b}, V) \in \mathcal{P} : \exists P \in \GL_{m}. \; \exists b'. \; (P^{-1}, I_n) A_{a, b} (P, I_n) = A_{e_{wr}, b'} \}.\]

\begin{lemma}
    \(\mathcal{P} = \bigcup_{1 \leq w \leq k, 0 \leq r \leq \lambda_{w}} \mathcal{P}_{wr}\).
\end{lemma}
\begin{proof}
    Let \((A_{a, b}, (V_i)) \in \mathcal{P}\).
    Define \(f : \mathbb{C}^m \to \mathbb{C}\) by \(f(e_{ij}) = a_{ij}\).
    We take the coordinate-free view of \(A_{a,b}\) as the linear transformation \(T\) that sends \(e_{ij}\) to \(e_{i,j - 1} + f(e_{ij})\), sends \(f_{i + 1}\) to \(f_i\), and sends \(f_1\) to \((b, 0)\).
    Since \(X\) is nilpotent, \cref{normalization} says that there is a change of basis \(P\) such that \(Pe_{ij}\) forms a Jordan basis for \(X\), and for all but one pair \((i, j)\) we have \(f(Pe_{ij}) = 0\).
    \par Because \(Pe_{ij}\) is a Jordan basis, \(A_{a',b'}\) of \(T\) in this new basis has the same upper-left corner \(X\); and since \(f(Pe_{ij}) = 0\) for all but one pair \((i,j)\), we have \(a'_{ij} = 0\) for all but one pair \((i,j)\).
\end{proof}
\begin{lemma}
    \(\mathcal{P}_{w_1r_1} = \mathcal{P}_{w_2r_2}\) exactly when either \(r_1 = r_2 = 0\), or \((\lambda_{w_1}, r_1) = (\lambda_{w_2}, r_2)\).
    When \(\mathcal{P}_{w_1r_1} \neq \mathcal{P}_{w_2r_2}\), we have \(\mathcal{P}_{w_1r_1} \cap \mathcal{P}_{w_2r_2} = \emptyset\). 
\end{lemma}
\begin{proof}
    Now, if \(r_1 = r_2 = 0\), we have \(e_{wr_1} = e_{wr_2} = 0\), so clearly \(\mathcal{P}_{wr_1} = \mathcal{P}_{wr_2}\).
    And if \((\lambda_{w_1}, r_1) = (\lambda_{w_2}, r_2)\), then clearly any matrix of the form \(A_{e_{w_1r_1}b'}\) can be transformed to a matrix of the form \(A_{e_{w_2r_2}b'}\) by making the change of basis that swaps \(e_{w_1j}\) with \(e_{w_2j}\).
    Conversely, suppose \(\mathcal{P}_{w_1r_1} = \mathcal{P}_{w_2r_2}\).
    Then there is \(P\) such that \((P^{-1},I_n) A_{e_{w_1r_1},b} (P, I_n) = A_{e_{w_2r_2}, b'}\).

    Then, either there is \(k\) such that (for any \(b_1',b_2'\)) \(A_{e_{w_1r_1}b_1'}^k \)
    \par The second observation, that the nonequal sets are disjoint, just follows from the fact that (this restricted form of) similarity is an equivalence relation.
\end{proof}

Now, fix any \(w\) and \(r\).
We will find the irreducible components of \(\mathcal{P}_{w,r}\).
These will all happen to be equidimensional (with dimensions independent of \(w\) and \(r\)), so their closures in \(\mathcal{P}\) will be the irreducible components of \(\mathcal{P}\).
\par Let 
\[G := \{P \in \GL_m : P^{-1} X P = X\},\]
and
\[G_{wr} = \{A \in G : e_{wr}A = e_{wr}\}.\]
\par Now, define 
\[\mathcal{Q}_{wr} = \{(A_{e_{wr}, b}, (V_i)) \in \mathcal{P}_{wr}\}.\]
Let \(G\) act on \(\mathcal{P}_{wr}\) by
\[P \cdot (A_{e_{wr},b}, (V_i)) := ((P, I_n) A_{e_{wr}, b} (P, I_n)^{-1}, (P, I_n)(V_i)) = (A_{e_{wr} P^{-1}, Pb}, (P, I_n) (V_i)).\]
Note that for any \(x \in \mathcal{Q}_{wr}\), we have \(G_{wr} = \{g \in G : g \cdot x \in \mathcal{Q}_{wr}\}\).
So, by restriction of \(G\) to \(G_{wr}\) and \(\mathcal{P}_{wr}\) to \(\mathcal{Q}_{wr}\), we obtain an action of \(G_{wr}\) on \(\mathcal{Q}_{wr}\).
\par Consider the map \(\varphi : \mathcal{Q}_{wr} \times G \to \mathcal{P}_{wr}\) defined by
\[(x, P) \mapsto P \cdot x.\]
Then, letting \(G_{wr}\) act on \(G\) by \(g \cdot h := hg^{-1}\), we obtain an action of \(G_{wr}\) on \(\mathcal{Q}_{wr} \times G\).

\begin{lemma}
    The map \(\varphi\) is a principal \(G_{wr}\)-bundle.
\end{lemma}
\begin{proof}
    We need to show that \(G_{wr}\) acts freely and transitively on the fibers of \(\varphi\).
    It is obvious that \(G_{wr}\) acts freely on \(\mathcal{Q}_{wr} \times G\); it is enough to note that it acts freely on \(G\).
    \par Let \(y \in \mathcal{P}_{wr}\).
    By definition of \(\mathcal{P}_{wr}\), there is \(P_y \in G\) with \(P_y \cdot y \in \mathcal{Q}_{wr}\).
    We have
    \begin{align*}
        \varphi^{-1}(y) = \{(x, P) : P \cdot x = y \} & = \\
        \{(P^{-1} y, P) : P^{-1} y \in \mathcal{Q}_{wr} \} & = \\
        \{((P^{-1}P_y^{-1}) \cdot (P_y \cdot y), P) : (P^{-1} P_y^{-1}) \cdot (P_y \cdot y) \in \mathcal{Q}_{wr}\} & .
    \end{align*}
    Using that \(G_{wr} = \{g \in G : g \cdot (P_y \cdot y) = P_y \cdot y\}\), the expression above becomes
    \[\{((P^{-1}P_y^{-1}) \cdot (P_y \cdot y), P) : (P^{-1} P_y^{-1}) \in G_{wr}\}.\]
    Setting \(Q := P^{-1} P_y^{-1}\), so that \(P = P_y^{-1} Q^{-1}\), the above becomes
    \begin{align*}
        \{(Q \cdot (P_y \cdot y), P_y^{-1} Q^{-1}) : Q \in G_{wr}\} & = \\
        \{Q \cdot (P_y \cdot y, P_y^{-1}) : Q \in G_{wr}\} & .
    \end{align*}
        We see that the fibers are exactly the \(G_{wr}\)-orbits; or in other words, \(G_{wr}\) acts transitively on the fibers, as desired.
    % Because \(X\) and \(G\) are irreducible, \(X \times G\) is irreducible, and therefore \(\varphi(X \times G)\) is, as well.
    % Let \(X_1, ..., X_k\) be the irreducible components of \(\mathcal{Q}_{wr}\).
    % It is clear that \(V_{mr} = \bigcup_i \overline{\varphi(X_i \times G)}\).
    % And if we take \(x \in X_i \setminus \bigcup_{j \neq i} X_j\), it is clear (since \(G\) acts faithfully on itself) that \(\varphi(x, I_m)\)
    % All that remains is to show that \(X_i \notin \bigcup_{j \neq i} \overline{\varphi(X_i \times G)}\).
    % Indeed, 
\end{proof}
Our strategy is to find the irreducible components \(X \subseteq \mathcal{Q}_{wr}\), and we will then argue that the irreducible components of \(\mathcal{P}_{wr}\) are of the form \(\varphi(X \times G)\).
So, we will now find the irreducible components of \(\mathcal{Q}_{wr}\).
\par Actually \(\mathcal{Q}_{wr}\) might be unnecessarily difficult to think about; it is easiest in the case \(r = 0\).
So, we will change basis to make \(r = 0\).
Let \(m' = m - r\), and \(n' = n + r\).
Let \(\lambda' = (\lambda_1, ..., \lambda_{w - 1}, \lambda_w - r, \lambda_{w + 1}, ..., \lambda_k)\).
Let \(X' = J(\lambda)\).
Let \(\mathcal{Q}' = \{(A_{X', 0, b}, (V_i)) : \forall i. \; A_{X', 0, b} V_{i + 1} \subseteq V_i\}\).
\begin{lemma}
    \(\mathcal{Q}_{wr} \cong \mathcal{Q}'\)
\end{lemma}
\begin{proof}
    Let \(e_{ij}'\) be the standard Jordan basis for \(X'\).
    Let \(f_1', ..., f_{n'}'\) be a basis for \(\mathbb{C}^n\), with \(A_{X', 0, b} f_{i + 1}' = f_i'\), and \(A_{X', 0, b} f_1 = (b, 0)\).
    Set \(m' := m - r\), and \(n' := n + r\).
    Define the linear map \(Q_{wr} : \mathbb{C}^{m' + n'} \to \mathbb{C}^{m + n}\) by:
    \begin{itemize}
        \item For all \(i\), \(e_{ij}' \mapsto e_{ij}\).
        \item For \(j = 1, ..., r\), \(f_{n + j}' \mapsto e_{w,(\lambda_w - r) + j}\).
        \item For \(j = 1, ..., n\), \(f_j' \mapsto f_j + e_{w, (\lambda_w - r) - n + j}\).
    \end{itemize}
    % The inverse map \(Q_{wr}^{-1} : \mathbb{C}^{m + n} \to \mathbb{C}^{m' + n'}\) is
    % \begin{itemize}
    %     \item For all \(i \neq w\), \(e_{ij} \mapsto e_{ij}'\).
    %     \item For \(j = 1, ..., \lambda_w - r\), \(e_{wj} \mapsto e_{wj}'\).
    %     \item For \(j = 1, ..., r\), \(e_{wj} \mapsto f'_{n + j}\).
    %     \item For \(j = 1, ..., n\), \(f_j \mapsto f'_j - e_{w,\lambda_w - n + r}\)
    % \end{itemize}
    Observe that conjugation by \(Q_{wr}\) maps \(\mathcal{Q}_{wr}\) to \(\mathcal{Q}'\), and conjugation by \(Q_{wr}^{-1}\) maps \(\mathcal{Q}'\) to \(\mathcal{Q}_{wr}\).
\end{proof}
In the next section we will find the irreducible components of \(\mathcal{Q}'\).
To avoid death by primes, we will refer to it as \(\mathcal{Q}\), and refer to \(m',n'\) as \(m,n\), and so on, throughout the next section.

\section{The components of \(\mathcal{Q}\)}
\subsection{Setup}
Let \(X\) be nilpotent with Jordan basis \((e_{ij}')_{1 \leq i \leq k, 1 \leq j \leq \lambda_i'}\).
Let \(V = \{(A_{a,b}, U) : \forall i. \; A_{a,b} U_{i + 1} \subseteq U\}\).
Let
\[U_0 = \{(A_{0,b}, U) \in V\}.\]
We will find the irreducible components of \(U_0\).
\par We write \(b_{ij}\) to denote the projection of \(b \in \mathbb{C}^m\) onto \(e_{ij}\).
For each row \(i\), let \(p_i(b) = \max\{j : b_{ij} \neq 0\}\) (the maximum of the empty set is zero).
Then set \(q_i(b) = \lambda_i - p_i(b)\).
When it is clear enough from context, we will just write \(p_i\) and \(q_i\) instead of \(p_i(b)\) and \(q_i(b)\).
\par Let \(I = \{i_1 < \cdots < i_r\} \subseteq \{1, ..., k\}\), and let \((\rho_i)_{i \in I}\) be any map \(I \to \mathbb{N}_{> 0}\) such that (1) \(\rho_i \leq \lambda_i\), (2) \(\rho_i\) is decreasing with \(i\), (3) \(\lambda_i - \rho_i\) is decreasing with \(i\), and (4) \(\rho_i < n\).
For notational convenience (although we assign meaning to neither \(i_0\) nor \(i_{r + 1}\)), we define \(q_{i_0} := n\), and \(p_{i_{r + 1}} := 0\).
Then, we define \(B_{I, (\rho_i)}\) as the set of \(b \in \mathbb{C}^m\) satisfying the following conditions.
\begin{itemize}
    \item For all \(k \in \{1, ..., r\}\), \(p_i = \rho_i\).
    \item For all \(k \in \{0, ..., r\}\), \(p_{i_{k + 1}} = \max_{i : q_i < q_{i_k}} p_i\).
\end{itemize}
Note that for any \(b \in B_{I, (\rho_i)}\) we have \(p_{i_1} > \cdots > p_{i_r} > p_{i_{r + 1}}\), and also \(q_{i_0} > q_{i_1} > \cdots > q_{i_r}\).
\par Let \(U_{I, (\rho_i)} := \{(A_{0, b}, U) \in V : b \in B_{I, (\rho_i)}\}\).
The idea here is to show that blah relation holds between U and B.

\begin{lemma}\label{bs_union}
    \(\mathbb{C}^m = \bigcup_{I, (\rho_i)} B_{I, (\rho_i)}\), where \(I\) ranges over all subsets of \(\{1, ..., k\}\), and \((\rho_i)\) ranges over all maps \(I \to \mathbb{N}_{>0}\) satisfying the conditions (1),(2),(3),(4).
    Further, none of the \(B_{I, (\rho_i)}\) is contained in the union of the others.
\end{lemma}
\begin{proof}
    Let \(b \in \mathbb{C}^m\).
    If \(\{i : q_{i_0} > q_i\}\) is the empty set, then stop.
    Otherwise, take any \(i_1 \in \argmax_{i : q_{i_0} > q_i} p_i\), and set \(\rho_{i_1} := p_{i_1}\).
    If \(\{i : q_{i_1} > q_i\} = \emptyset\), then stop.
    Otherwise, take any \(i_2 \in \argmax_{i : q_{i_1} > q_i} p_i\), and set \(\rho_{i_2} := p_{i_2}\).
    Continuing on in this way, eventually we reach a point where \(\{i : q_{i_k} > q_i\} = \emptyset\).
    Then we set \(I = \{i_1, ..., i_k\}\).
    Note that \(I, (\rho_i)\) satisfy conditions (1)--(4), and furthermore \(b \in B_{I, (\rho_i)}\).
    \par Now, we show that no \(B_{I, (\rho_i)}\) is contained in the union of the others.
    Indeed, fix \(I\) and \((\rho_i)\).
    Take any \(b \in B_{I, (\rho_i)}\) with \(p_i = \rho_i\) for \(i \in I\) and \(p_i = 0\) for \(i \notin I\).
    It is clear that \(b \notin B_{I', (\rho_i')}\) whenever \(I' \neq I\) or \((\rho_i') \neq (\rho_i)\).
\end{proof}

\subsection{A study of \(B_{I, (\rho_i)}\)}
Fix any \(I\) and \((\rho_i)\) satisfying the conditions (1)--(4).
As before, we write \(\{i_1 < \cdots < i_r\} := I\), and \(q_{i_0} := n\), and \(p_{i_{r + 1}} := 0\).

\par First, we provide an alternative characterization of \(B_{I, (\rho_i)}\).
\begin{lemma}\label{alternative_bs_one}
    \(B_{I, (\rho_i)}\) is the set of \(b \in \mathbb{C}^m\) satisfying the following conditions.
    \begin{itemize}
        \item For all \(k \in \{1, ..., r\}\), \(p_{i_k} = \rho_{i_k}\).
        \item For all \(i \notin I\),
        \begin{itemize}
            \item For all \(k \in \{0, ..., r\}\) such that \(\lambda_i > q_{i_k} + p_{i_{k + 1}}\), we have \(q_{i_k} \leq q_i\).
            \item For all \(k \in \{1, ..., r + 1\}\) such that \(\lambda_i \leq q_{i_{k - 1}} + p_{i_k}\), we have \(p_i \leq p_{i_k}\).
        \end{itemize}
    \end{itemize}
\end{lemma}
\begin{proof}
    First we show that every element of \(B_{I, (\rho_i)}\) satisfies those conditions.
    Let \(b \in B_{I, (\rho_i)}\).
    It is clear that \(\forall k. \; p_{i_k} = \rho_{i_k}\).
    \par Take \(i \notin I\) and \(k \in \{0, ..., r\}\) such that \(\lambda_i > q_{i_k} + p_{i_{k + 1}}\).
    Suppose for contradiction that \(q_i < q_{i_k}\).
    Then \(p_i \leq \max_{j : q_j < q_{i_k}} p_j = p_{i_{k + 1}}\).
    Then \(\lambda_i = p_i + q_i < q_{i_k} + p_{i_{k + 1}}\), a contradiction.
    So we must have \(q_{i_k} \leq q_i\), as desired.
    \par Now take \(i \notin I\) and \(k \in \{1, ..., r + 1\}\) such that \(\lambda_i \leq q_{i_{k - 1}} + p_{i_k}\).
    Suppose for contradiction that \(p_i > p_{i_k}\).
    Then, putting this together with the first inequality, \(\lambda_i - p_i < q_{i_{k - 1}} + p_{i_k} - p_{i_k}\); that is, \(q_i < q_{i_{k - 1}}\).
    Consequently, \(p_i \leq \max_{j : q_j < q_{i_{k - 1}}} p_j = p_{i_k}\), as desired.
    \par Now we have shown that every element of \(B_{I, (\rho_i)}\) satisfies the conditions of the lemma, and we proceed to the converse.
    Let \(b \in \mathbb{C}^m\) satisfy the conditions.
    Let \(k \in \{0, ..., r\}\).
    We need to show that \(\max_{j : q_j < q_{i_k}} p_j = p_{i_{k + 1}}\).
    Given the conditions (1)--(4) on \(\rho_i\), it suffices to show that for each \(i \notin I\) with \(q_i > q_{i_k}\), we have \(p_i \leq p_{i_{k + 1}}\).
    Indeed, given \(i \notin I\) with \(q_i > q_{i_k}\), we cannot have \(\lambda_i > q_{i_k} + p_{i_{k + 1}}\), as that would imply that \(q_{i_k} \leq q_i\).
    Hence we must have \(\lambda_i \leq q_{i_{k - 1}} + p_{i_k}\), and consequently \(p_i \leq p_{i_k}\).
\end{proof}

\begin{corollary}\label{alternative_bs_two}
    \(B_{I, (\rho_i)}\) is the set of \(b \in \mathbb{C}^m\) satisfying the following conditions.
    \begin{itemize}
        \item For all \(k \in \{1, ..., r\}\), \(p_{i_k} = \rho_{i_k}\).
        \item For \(i \notin I\),
        \begin{itemize}
            \item If \(\lambda_i \geq q_{i_0} + p_{i_1}\), then \(p_i \leq \lambda_i - q_{i_0}\).
            \item If there is \(k \in \{1, ..., r\}\) with \(q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\), then \(p_i \leq \min(p_{i_k}, \lambda_i - q_{i_k})\).
            \item If \(q_{i_r} + p_{i_{r + 1}} > \lambda_i\), then \(p_i \leq p_{i_{r + 1}}\).
        \end{itemize}
    \end{itemize}
\end{corollary}
\begin{proof}
    Both \(q_{i_k}\) and \(p_{i_k}\) are decreasing as \(k\) increases, so this follows directly from \cref{alternative_bs_one}.
    (Note that \(p_i \leq \lambda_i - q_{i_k}\) iff \(q_{i_k} \leq q_i\).)
\end{proof}

\begin{corollary}\label{bs_iso}
    \[B_{I, (\rho_i)} \cong \prod_{k = 1}^r (\mathbb{C}^{\rho_{i_k} - 1} \times (\mathbb{C} \setminus \{0\})) \times \prod_{i : \lambda_i \geq q_{i_0} + p_{i_1}} \mathbb{C}^{\lambda_i - q_{i_0}} \times \prod_{k = 1}^r \prod_{\{i \notin I : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}}\mathbb{C}^{\min(p_{i_k}, \lambda_i - q_{i_k})}\]
\end{corollary}
\begin{proof}
    Here I use the notation \(x \times y = (x, y)\), and so on.
    The isomorphism sends \(b \in B_{I, (\rho_i)}\) to 
    \[\prod_{k = 1}^{r}(b_{i_k1},..., b_{i_k\rho_{i_k}}) \times \prod_{i : \lambda_i \geq q_{i_0} + p_{i_1}} (b_{i1}, ..., b_{i, \lambda_i - q_{i_0}}) \times\]
    \[\prod_{k = 1}^r \prod_{\{i \notin I : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} (b_{i1}, ..., b_{i,\min(p_{i_k}, \lambda_i - q_{i_k})}).\]
    \cref{alternative_bs_two} tells us that this is an isomorphism.
\end{proof}

\begin{corollary}\label{bs_dim}
    \[\dim B_{I, (\rho_i)} = \sum_{i : \lambda_i \geq q_{i_0} + \rho_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^r \sum_{\{i : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} \min(p_{i_k}, \lambda_i - q_{i_k}).\]
\end{corollary}
\begin{proof}
    Immediate from \cref{bs_iso}.
\end{proof}

\subsection{A study of \(U_{I, (\rho_i)}\)}
Fix any \(I\) and \((\rho_i)\) satisfying the conditions of Lemma 8.1.
We will find the irreducible components of \(U_{I, (\rho_i)}\), and show that their closures are in fact irreducible components of \(U_0\).
\par I claim that \(A_{0,b}\) has the same shape for every \((A_{0, b}, U) \in U_{I, (\rho_i)}\).
Call this shape \(\mu\).
By finding a algebraic map taking \(b\) to a Jordan basis for \(A_{0,b}\), we will put \(U_{I, (\rho_i)}\) in isomorphism with the product (choice of \(b\)) \(\times\) (Springer fiber at \(J(\mu)\)).
Then we will use our result about the usual Springer fiber at \(J(\mu)\) to find the irreducible components of \(U_{I, (\rho_i)}\).
\par So, now we find a Jordan basis for \(A_{0, b}\).
\begin{lemma}
    The following vectors give a Jordan basis for \(A_{0,b}\).
    (For convenience, we write \(A := A_{0,b}\) in this lemma and proof.)
    \begin{itemize}
        \item For \(i \notin I\), the chain of length \(p_i + q_i\) beginning with \(e_{i, p_i + q_i}\)
        \item The chain of length \(n + p_{i_1}\) beginning with \(f_n - (A^{n + p_{i_1}}f_n \rightshift n + p_{i_1})\)
        \item For \(k \in \{1, ..., r\}\), the chain of length \(q_{i_k} + p_{i_{k + 1}}\) beginning with \(v_{i_k} - (A^{q_{i_k} + p_{i_{k + 1}}} v_{i_k} \rightshift q_{i_k} + p_{i_{k + 1}})\), where \(v_{i_k} := A^{n - q_{i_k}} f_n - \sum_{l = 1}^{p_{i_k}} b_{i_kl} e_{i,l + q_{i_k}}\)
    \end{itemize}
\end{lemma}
\begin{proof}
    There are three things to check: (1) the chains are no longer than their claimed lengths, (2) the claimed lengths sum to \(m + n\), and (3) the span of the chains is \(\mathbb{C}^{m + n}\).
    \begin{proof}[Proof of (1)]
        It is obvious that a chain beginning with \(e_{i, p_i + q_i}\) has length \(p_i + q_i\).
        \par Now consider the chain beginning with \(f_n - (A^{n + p_{i_1}} f_n \rightshift n + p_{i_1})\).
        Note that \(A^n f_n = b\), so \(A^{n + p_{i_1}} f_n = A^{p_{i_1}} b = b \leftshift p_{i_1}\).
        By shifting \(b\) left \(p_{i_1}\) times, we zero out all the rows \(i\) where \(q_i < n\).
        This ensures that the operation of shifting \(b \leftshift p_{i_1}\) right \(n + p_{i_1}\) times is invertible by shifting left \(n + p_{i_1}\) times.
        That is,
        \begin{align*}
            A^{n + p_{i_1}} f_n & = \\
            b \leftshift p_{i_1} & = \\
            ((b \leftshift p_{i_1}) \rightshift n + p_{i_1}) \leftshift n + p_{i_1} & = \\
            A^{n + p_{i_1}}(A^{n + p_{i_1}}f_n \rightshift n + p_{i_1}) & .
        \end{align*}
        This shows that the chain has length at most \(n + p_{i_1}\), as desired.
        \par Now, let \(k \in \{1, ..., r\}\).
        We have \(v_{i_k} = A^{n - q_{i_k}} f_n - \sum_{l = 1}^{p_{i_k}} b_{i_kl} e_{i_k,l + q_{i_k}}\).
        First we note that \(q_{i_k} < n\) by the definition of \(U_{I, (\rho_i)}\), so the definition of \(v_{i_k}\) makes sense.
        We consider the chain beginning with \(v_{i_k} - (A^{q_{i_k} + p_{i_{k + 1}}} v_{i_k} \rightshift q_{i_k} + p_{i_{k + 1}})\).
        Note that \(A^{q_{i_k}} v_{i_k}\) is just \(b\) with row \(i_k\) zeroed out.
        For brevity, we write \(b_{i_k} := A^{q_{i_k}} v_{i_k}\).
        Note that \(b_{i_k} \leftshift p_{i_{k + 1}}\) has rows \(l\) zeroed out, for all \(l\) with \(q_l < q_{i_k}\).
        This ensures that shifting \(b_{i_k} \leftshift p_{i_{k + 1}}\) right \(q_{i_k} + p_{i_{k + 1}}\) times can be inverted by shifting left \(q_{i_k} + p_{i_{k + 1}}\) times.
        That is,
        \begin{align*}
            A^{q_{i_k} + p_{i_{k + 1}}} v_{i_k} & = \\
            b_{i_k} \leftshift p_{i_{k + 1}} & = \\
            ((b_{i_k} \leftshift p_{i_{k + 1}}) \rightshift q_{i_k} + p_{i_{k + 1}}) \leftshift q_{i_k} + p_{i_{k + 1}} & = \\
            A^{q_{i_k} + p_{i_{k + 1}}}(A^{q_{i_k} + p_{i_{k + 1}}} v_{i_k} \rightshift q_{i_k} + p_{i_{k + 1}}) & .
        \end{align*}
        This shows that the chain has length at most \(q_{i_k} + p_{i_{k + 1}}\), as desired.
    \end{proof}
    \begin{proof}[Proof of (2)]
        The sum of the lengths is
        \[\sum_{i \notin I} (p_i + q_i) + (n + P) + \sum_{i \in I} (q_i + P_i).\]
        Writing \(I = \{i_1 < \cdots < i_{|I|}\}\), we note that \(P = p_{i_1}\), that \(P_{i_{|I|}} = 0\), and that for \(l < |I|\) we have \(P_{i_l} = p_{i_{l + 1}}\).
        Thus, the sum above is 
        \[\sum_{i \notin I} (p_i + q_i) + (n + p_{i_1}) + \sum_{l = 1}^{|I|} (q_{i_l} + p_{i_{l + 1}}) = \]
        \[\sum_{i \notin I} (p_i + q_i) + n + \sum_{l = 1}^{|I|} (q_{i_l} + p_{i_l}) = n + \sum_i (q_i + p_i) = m + n.\]
    \end{proof}
    \begin{proof}[Proof of (3)]
        Let \(W\) be the span of the chains listed.
        We need to show that \(W = \mathbb{C}^{m + n}\).
        Because every \(i \in I\) satisfies \(q_i < n\), clearly \(\langle e_{ij}\rangle_{i,j : q_i \geq n} \subseteq W\).
        \par I claim that \(f_n \in W\) as well.
        To see this, we consider the chain beginning with \(f_n - (A^{n + P} f_n \rightshift n + P)\).
        As explained in the proof of (1), we have \(A^{n + P} f_n \in \langle e_{ij}\rangle_{i,j : q_i \geq n}\).
        Consequently, \((A^{n + P} f_n \rightshift n + P) \in \langle e_{ij} \rangle_{i,j : q_i \geq n} \subseteq W\).
        Because \(f_n - (A^{n + P} f_n \rightshift n + P) \in W\), this implies that \(f_n \in W\).
        \par Because \(AW \subseteq W\) (obvious, since \(W\) is the span of chains), the fact that \(f_n \in W\) implies that \(f_i \in W\) for each \(i\), and also \(b \leftshift l \in W\) for each \(l \geq 0\).
        \par Now we are left with showing that \(\langle e_{ij} \rangle_{i,j : q_i < n} \subseteq W\).
        This is obvious for \(i \notin I\).
        For \(i \in I\), we do it inductively.
        Fix \(i \in I\), and suppose we have already shown that for all \(i' \in I\) with \(q_{i'} > q_i\), we have \(\langle e_{ij} \rangle_{j \textrm{ arbitrary}} \subseteq W\).
        We will show that \(\langle e_{ij} \rangle_{j \textrm{ arbitrary}} \subseteq W\).
        \par To see this, we consider the chain beginning with \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i)\).
        (Recall \(v_i = A^{n - q_i} f_n - \sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i}\).)
        Because \(AW \subseteq W\), and \(b_{i,p_i} \neq 0\) (by definition of \(p_i\)), it suffices to show that \(\sum_{l = 1}^{p_i} b_{il} e_{i,k + q_i} \in W\).
        As explained in the proof of (1), we have \(A^{q_i + P_i} v_i \in \langle e_{lj} \rangle_{q_l \geq q_i}\).
        And since \(A^{q_i + P_i} v_i\) has row \(i\) zeroed out, in fact \(A^{q_i + P_i} v_i \in \langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i}\).
        Hence, \(A^{q_i + P_i} v_i \rightshift q_i + P_i \in \langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i}\).
        By our inductive hypothesis, \(\langle e_{lj} \rangle_{l : l \neq i \land q_l \geq q_i} \subseteq W\), and consequently \(A^{q_i + P_i} v_i \rightshift q_i + P_i \in W\).
        Since we know \(v_i - (A^{q_i + P_i} v_i \rightshift q_i + P_i) \in W\), this implies that \(v_i \in W\).
        Because \(A^{n - q_i} f_n \in W\), this then implies that \(\sum_{l = 1}^{p_i} b_{il} e_{i,l + q_i} \in W\), as desired.
    \end{proof}
    We proved (1), (2), (3), so we are done.
\end{proof}

Let \(\mu(I, (\rho_i))\) be the shape of the Jordan basis given in the previous lemma.
Let \(X_\mu\) be the Springer fiber at \(J(\mu)\), and let \((X_{\mu, \alpha})_{\alpha \in \mathrm{SYT}(\mu)}\) be the irreducible components.
\par Given a zero-indexed list \(L = [L_0, ..., L_{l - 1}]\), we define \(\gamma(L) = \sum_i i L_i\).
We are interested in this thing because the dimension of \(X_\mu\) is \(\gamma([\mu_1, ..., \mu_l])\).

\begin{lemma}
    \[\gamma([\mu_1, ..., \mu_l]) = \gamma([0, \lambda_1, ..., \lambda_k]) - \] 
    \[\left[\sum_{i : \lambda_i \geq q_{i_0} + \rho_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^r \sum_{\{i : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} \min(p_{i_k}, \lambda_i - q_{i_k})\right].\]
\end{lemma}
\begin{proof}
    Let \(L = [q_{i_0}, \lambda_1, ..., \lambda_k]\).
    Note that \(\mu = [..., q_{i_0} + p_{i_1}, ..., q_{i_1} + p_{i_2}, ..., ..., q_{i_r} + p_{i_{r + 1}}, ...]\).
    Let \(L'\) be the result of taking \(\mu\) and, for each \(x\), replacing one occurence of \(q_{i_x} + p_{i_{x + 1}}\) by \(q_{i_x} + p_{i_x}\); that is,
    \(L' = [..., q_{i_0}, ..., q_{i_1} + p_{i_1}, ..., q_{i_2} + p_{i_2}, ..., q_{i_r} + p_{i_r}]\).
    Note that we get from \(\mu\) to \(L'\) by just moving each \(p_{i_k}\) to the right by \(1 + \#\{i \notin I : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}\) slots.
    So,
    \[\gamma(L') - \gamma(\mu) = \sum_{k = 1}^r p_{i_k} (1 + \#\{i \notin I : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}) = \]
    \[\sum_{k = 1}^r p_{i_k} \cdot \#\{i : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}.\]
    Now we consider how to get from \(L'\) to \(L\).
    First we shift \(q_{i_0}\) to the left by \(\#\{i \notin I : \lambda_i \geq q_{i_0} + p_{i_1}\}\) slots.
    Then we leave \(q_{i_0}\) in place and sort the rest of the list.
    This entails shifting each \(q_{i_x} + p_{i_x}\) to the left by \(\#\{i \notin I : q_{i_x} + p_{i_x} > \lambda_i \geq q_{i_x} + p_{i_{x + 1}}\}\) slots.
    Shifting \(q_{i_x} + p_{i_x}\) to the left one slot, by swapping it with \(\lambda_i\), changes the value of \(\gamma\) by \(\lambda_i - (q_{i_x} + p_{i_x})\).
    To go from \(L'\) to \(L\), we can just make these swaps repeatedly.  
    So,
    \[\gamma(L) - \gamma(L') = \]
    \[\sum_{i \notin I : \lambda_i \geq q_{i_0} + p_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^s \sum_{\{i \notin I : q_{i_k} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} (\lambda_i - (q_{i_k} + p_{i_k})).\]
    \par Now,
    \begin{align*}
        \gamma(L) - \gamma(\mu) & = \\
        [\gamma(L) - \gamma(L')] + [\gamma(L') - \gamma(\mu)] & = \\
        \left[\sum_{i : \lambda_i \geq q_{i_0} + p_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^s \sum_{\{i : q_{i_k} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} (\lambda_i - (q_{i_k} + p_{i_k}))\right] & + \\
        \left[\sum_{k = 1}^s \sum_{\{i : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} p_{i_k}\right] & = \\
        \sum_{i : \lambda_i \geq q_{i_0} + p_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^s \sum_{\{i : q_{i_{k - 1}} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_k}\}} p_{i_k} & + \\
        \sum_{k = 1}^s \sum_{i : q_{i_k} + p_{i_k} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}} (\lambda_i - q_{i_k}) & = \\
        \sum_{i : \lambda_i \geq q_{i_0} + p_{i_1}} (\lambda_i - q_{i_0}) + \sum_{k = 1}^s \sum_{\{i : q_{i_{k - 1} + p_{i_k}} > \lambda_i \geq q_{i_k} + p_{i_{k + 1}}\}} \min(p_{i_k}, \lambda_i - q_{i_k}) & .
    \end{align*}    
\end{proof}

\begin{lemma}\label{u_i_rho_iso}
    \(U_{I, (\rho_i)} \cong B_{I, (\rho_i)} \times X_\mu\).
\end{lemma}
\begin{proof}
    For \(b \in B_{I, (\rho_i)}\), let \(P_b\) be the change-of-basis matrix, with columns given by the Jordan basis of the previous lemma, so that \(J(\mu) = P_b^{-1} A_{0,b} P_b\).
    From looking at the Jordan basis of the previous lemma (the basis can be expressed in terms of \(A, \lambda, e_{ij}, f_i, I, \rho_i, b\)), it is clear that the map \(P : B_{I, (\rho_i)} \to \GL_{m + n}\), given by \(b \mapsto P_b\), is algebraic.
    \par Now, we remark that the Springer fiber at \(A_{0,b}\) is simply \(\{P_b U : U \in X_\mu\}\).
    This gives us the isomorphism \(B_{I, (\rho_i)} \times X_\mu \to U_{I, (\rho_i)}\).
    \[(b, U) \mapsto (P_b J(\mu) P_b^{-1}, P_b U),\]
    with inverse
    \[(A_{0,b}, U) \mapsto (b, P_b^{-1} U).\]
\end{proof}

\begin{corollary}\label{u_i_rho_irred_and_dim}
    The 
    \(U_{I, (\rho_i)}\) is irreducible, and its dimension is \(\gamma([0, \lambda_1, ..., \lambda_k])\).
\end{corollary}
\begin{proof}
    Irreducibility follows from \cref{u_i_rho_iso}, since \(B_{I, (\rho_i)}\) is irreducible (by \cref{bs_iso}), as is \(X_\mu\).
    To get the dimension, we add the dimension of \(B_{I, (\rho_i)}\) to the dimension of \(X_\mu\).
    The dimension of \(X_\mu\) is 
\end{proof}

\subsection{Conclusion}
At last we have reached the base of our recursive procedure, and we begin to propagate upwards.
We write \((X_{\mu, \alpha})_\alpha\) to denote the irreducible components of \(X_\mu\).
\begin{lemma}
    The irreducible components of \(U_{I, (\rho_i)}\) are exactly the subvarieties 
    \[U_{I, (\rho_i), \alpha} := \{(P_b J(\mu) P_b^{-1}, P_b U) : b \in B_{I, (\rho_i)}, U \in X_{\mu, \alpha}\}.\]
    These are distinct.  
    There are blah of them, and each has dimension blah.
\end{lemma}
\begin{proof}
    We know from \cref{bs_iso} that \(B_{I, (\rho_i)}\) is irreducible.
    So, the components of \(B_{I, (\rho_i)} \times X_\mu\) are obviously \(B_{I, (\rho_i)} \times X_{\mu, \alpha}\).
    Their dimensions are blah, and there are... of them.
    taking image under the isomorphism gives the desired result.
\end{proof}

\begin{lemma}
    The irreducible components of \(U_0\) are exactly the \(U_{I, (\rho_i), \alpha}\).
    They are distinct.
    There are blah of them, each of dimension blah.
\end{lemma}

\section{Finding the irreducible components of a springer fiber at a slodowy slice}
Apply things from the previous two sections, and conclude.

\section{The components of \(S \times_\mathfrak{g} \widetilde{\mathcal{N}}\)}
Define \(R = \{(X, \mathfrak{b}_1, \mathfrak{b}_2) : X \in \mathfrak{b}_1 \land u(X) \in \mathfrak{b}_2\} \subseteq \mathcal{S}_{m,n}' \times \mathcal{B}_{m + n} \times \mathcal{B}_m\).
We can obtain a subvariety of \(R\) by requiring that \(u(X)\) is in some fixed similarity class.
We expect that each of these subvarieties is an irreducible component of dimension \(m^2\).
We will verify these things using our prvious computations of springer fibers.

\section{Linear Algebra Facts}
In this section we prove linear algebra facts that were used in the paper.
They are confined to this section to avoid interrupting the flow of the paper.

\subsection{The centralizer of a nilpotent matrix}
\begin{definition}
    A matrix \(Y\) is \emph{Toeplitz} if it is constant along bands parallel to the main diagonal.
    That is, \(\forall i,j,k. \; Y_{ij} = Y_{i + k, j + k}\).
\end{definition}
\begin{definition}
    An \(m \times n\) matrix \(Y\) is \emph{lower-left Toeplitz} if it is Toeplitz and, in addition, we have \(y_{n - i, j - 1} = 0\) whenever \(i + j \geq \min(m,n)\).
\end{definition}
That is, \(Y\) is lower-left Toeplitz if it is Toeplitz, and the only nonzero entries are those with Manhattan distance less than \(\min(m,n)\) from the entry in the bottom-left corner.
In yet other words, all but the leftmost (equivalently, bottommost) \(\min(m,n)\) diagonal bands are zero.

\begin{lemma}
    Let \(\lambda = (\lambda_1, ..., \lambda_k)\) be a partition of \(m\).
    The centralizer of \(J(\lambda)\) in \(\gl_m\) is the subalgebra consisting of matrices 
    \[M = \begin{pmatrix}
        M_{11} & \cdots & M_{1k}\\
        \vdots &        & \vdots\\
        M_{k1} & \cdots & M_{kk}
    \end{pmatrix},\]
    where each \(M_{ij}\) is a \(\lambda_i \times \lambda_j\) matrix, such that each \(M_{ij}\) is lower-left Toeplitz.
\end{lemma}
\begin{proof}
    Let 
    \[M = \begin{pmatrix}
        M_{11} & \cdots & M_{1k}\\
        \vdots &        & \vdots\\
        M_{k1} & \cdots & M_{kk}
    \end{pmatrix}.\]
    We need to show that \(J(\lambda)M = MJ(\lambda)\) if and only if each \(M_{ij}\) is lower-left Toeplitz.
    \par We have 
    \[J(\lambda)M = \begin{pmatrix}
        J_{\lambda_1}M_{11} & \cdots & J_{\lambda_1}M_{1k}\\
        \vdots & & \vdots\\
        J_{\lambda_k}M_{k1} & \cdots & J_{\lambda_k}M_{kk}
    \end{pmatrix}, \textrm{ and } MJ(\lambda) = \begin{pmatrix}
        M_{11} J_{\lambda_1} & \cdots & M_{1k} J_{\lambda_k}\\
        \vdots & & \vdots\\
        M_{k1} J_{\lambda_1} & \cdots & M_{kk} J_{\lambda_k}
    \end{pmatrix}.\]
    So, we have \(J(\lambda)M = MJ(\lambda)\) if and only if \(\forall i,j. \; J_{\lambda_i} M_{ij} = M_{ij} J_{\lambda_j}\).
    Multiplying on the left by \(J_{\lambda_i}\) just shifts each row down by one, and multiplying on the right by \(J_{\lambda_j}\) shifts each column left by one.
    The matrices for which left-shifting gives the same result as down-shifting are exactly the lower-left Toeplitz matrices.
\end{proof}
For nilpotent \(X\) in Jordan form and \(a\) in `normalized' form (i.e., with at most one nonzero element, which is a one), we will find the centralizer of \(A_{X, a, b}\) in 
\[\{(A, I) : A \in \gl_m\} \subseteq \gl_{m + n}.\]
Note that an element of the form \((A, I)\) commutes with \(A_{X, a, b}\) if and only if \(A\) commutes with \(X\), and \(aA = a\), and \(Ab = b\).
So, we just have to find 
\[\{A \in \gl_m : AX = XA, aA = a, Ab = b\}.\]

It is just the set of \(A = [A_{ij}]_{ij}\), where each \(A_{ij}\) is lower-left-Toeplitz.
\par Let \(v_{ij}\) be the leftmost column of \(A_{ij}\), so that
\[A_{ij} = \begin{pmatrix}
    v_{ij} \downshift 0 & \cdots & v_{ij} \downshift [\lambda_j - 1]
\end{pmatrix}.\]
Since \(A_{ij}\) is lower-left-Toeplitz, \(v_{ij}\) is of the form 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) can be chosen freely.
Now we determine which matrices of this form satisfy \(aA = a\).
For simplicity, we will instead find the matrices \(A\) such that \(aA = 0\) (so that \(a(A + I) = a\)).
(Note that \(I \in C_1\), and \(C_1\) is closed under addition, so this is really the same thing.)
\par In the case that \(a = 0\), clearly every \(A\) works.
Otherwise, let \(i_0, j_0\) be such that \(a_{i_0,j_0} = 1\).
Now, clearly the constraint that \(aA = 0\) is just saying that the \((i_0,j_0)\)th row of \(A\) must be zero.
That is, for each \(j\) the \(j_0\)th row of \(A_{i_0j}\) must be zero.
This just requires that for each \(j\), we must have 
\[v_{ij} = \begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) can be chosen freely.
So, we have now found the set 
\[C_2 := \{A \in \gl_m : AX = XA, aA = a\}.\]
It is the matrices of the form \(I + A\), where \(A_{ij}\) is a block matrix of size \(\lambda_i \times \lambda_j\) with 
\[A_{ij} = T(v_{ij}) = T\begin{pmatrix}
    0\\
    v_{ij}'
\end{pmatrix},\]
where \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i, \lambda_j)}\) in the case \(i \neq i_0\) and \(v_{ij}' \in \mathbb{C}^{\min(\lambda_i - j_0, \lambda_j)}\) if \(i = i_0\).

\subsection{A `normalization' fact about Jordan bases}

\begin{lemma}\label{normalization_helper_one}
    For any finite-dimensional \(V\), nilpotent \(A : V \to V\), and linear \(f : V \to \mathbb{C}\), there is a Jordan basis \(e_{ij}\) for \(A\) such that there is at most one \(i\) such that there exists \(j\) such that \(f(e_{ij}) \neq 0\).
\end{lemma}
\begin{proof}
    For any Jordan basis \((e_{ij})_{ij}\) of \(A\), define
    \[S((e_{ij})_{1 \leq i \leq k, 1 \leq j \leq \lambda_i}) := \sum_i \begin{cases}-1, & \forall j. \; f (e_{ij}) = 0 \\ \lambda_i - \min\{j : f (e_{ij}) \neq 0\}, & \textrm{otherwise} \end{cases}.\]
    We proceed by induction on the measure \(S\).
    That is, let \((e_{ij})_{ij}\) be a Jordan basis for \(A\).
    Our inductive hypothesis is that if there exists a Jordan basis \((e_{ij}')_{ij}\) with \(S((e_{ij}')_{ij}) < S((e_{ij})_{ij})\), then we get the desired conclusion.
    \par Now, we have two cases.
    In the first case, \((e_{ij})_{ij}\) already satisfies the desired property.
    In this case we are done.
    In the other case, there exist \(i_1, j_1, i_2, j_2\) with \(i_1 \neq i_2\), and \(f(e_{i_1j_1}) \neq 0\), and \(f(e_{i_2j_2}) \neq 0\).
    We let \(j_1, j_2\) be minimal with this property, so that \(\forall j < j_1. \; f(e_{i_1j}) = 0\), and \(\forall j < j_2. \; f(e_{i_2j}) = 0\).
    Wlog, we assume that \(\lambda_{i_1} - j_1 \leq \lambda_{i_2} - j_2\).
    \par By our inductive hypothesis, all we need to do is find a Jordan basis \((e_{ij}')_{ij}\) with \(S((e_{ij}')_{ij}) < S((e_{ij})_{ij})\).
    This is what we do.
    Define \(e'_{ij}\) as follows.
    \begin{itemize}
        \item \(e_{i_1, \lambda_1}' := e_{i_1, \lambda_1} - \frac{f(e_{i_1, j_1})}{f(e_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}\)
        \item For \(j < \lambda_1\), \(e_{i_1, j}' := A^{\lambda_{i_1} - j} e_{i_1, \lambda_{i_1}}'\)
        \item For \(i \neq i_1\), \(e_{ij}' := e_{ij}\).
    \end{itemize}
    Clearly this is a Jordan basis for \(A\).
    Further, I claim that \(S((e'_{ij})_{ij})  < S((e_{ij})_{ij})\).
    It suffices to show that \(\forall j \leq j_1. \; f (e_{i_1,j}) = 0\).
    We have 
    \[f (e_{i_1,j}') = f\left(A^{\lambda_{i_1} - j}\left(e_{i_1, \lambda_1} - \frac{f(e_{i_1, j_1})}{f(e_{i_2, j_2})}e_{i_2, j_2 + (\lambda_{i_1} - j_1)}\right)\right) =\]
    \[f\left(e_{i_1, j} - \frac{f(e_{i_1, j_1})}{f(e_{i_2, j_2})} e_{i_2, j_2 + (j - j_1)}\right) = f(e_{i_1, j}) - \frac{f(e_{i_1, j_1})}{f(e_{i_2, j_2})} f(e_{i_2, j_2 + (j - j_1)}).\]
    Clearly (by design), this expression is zero when \(j = j_1\).
    And for \(j < j_1\), we have \(f(e_{i_1, j}) = f(e_{i_2, j_2 + (j - j_1)}) = 0\), so it is zero then as well.
    Hence the measure \(S\) of this new basis is smaller, as desired.
\end{proof}

\begin{lemma}\label{normalization_helper_two}
    For any \(n\) and linear \(f : \mathbb{C}^n \to \mathbb{C}\), there is a Jordan basis \(e_j\) for \(J_n\) such that there is at most one \(j\) with \(f(e_j) \neq 0\).
\end{lemma}
\begin{proof}
    Let \(e_j\) be a Jordan basis for \(J_n\).
    If \(\{j : f(e_j) \neq 0\}\) is the empty set, we are done.
    Otherwise, let \(j_0 = \min\{j : f(e_j) \neq 0\}\).
    For any Jordan basis \(f_j\) with \(j_0 = \min\{j : f(e_j) \neq 0\}\), define 
    \[S((f_j)_j) := \begin{cases}
        -1, & \{j > j_0 : f(e_j) \neq 0\} = \emptyset \\
        n - \min\{j > j_0 : f(e_j) \neq 0\}, & \textrm{otherwise}
    \end{cases}.\]
    We proceed by induction on \(S\).
    That is, let \((e_j)_j\) be a Jordan basis for \(J_n\) with \(j_0 = \min\{j : f(e_j) \neq 0\}\).
    Our inductive hypothesis is that if there exists a Jordan basis \((e_j')_j\) with \(j_0 = \min\{j : f(e_j') \neq 0\}\) and \(S((e_j')_j) < S((e_j)_j)\), then the conclusion holds.
    \par We have two cases: either \((e_j)_j\) satisfies the desired property, or not.
    If not, then let \(j_1 = \min\{j > j_0 : f(e_j) \neq 0\}\), and define a new Jordan basis \(e_j'\) as follows.
    \begin{itemize}
        \item \(e_n' := e_n - \frac{f(e_{j_1})}{f(e_{j_0})} e_{n - (j_1 - j_0)}\)
        \item For \(j < n\), \(e_j' := J_n^{n - j} e_n'\)
    \end{itemize}
    It is straightforward to check that \(j_0 = \min\{j : f(e_j') \neq 0\}\), and that \(S((e_j')_j) \leq S((e_j)_j) - 1\).
    By our inductive hypothesis, we are done.
\end{proof}

\begin{theorem}\label{normalization}
    For any finite-dimensional \(V\), nilpotent \(A : V \to V\), and linear \(f : V \to \mathbb{C}\), there is a Jordan basis \(e_{ij}\) for \(A\) such that there is at most one pair \((i, j)\) with \(f(e_{ij}) \neq 0\).
\end{theorem}
\begin{proof}
    \cref{normalization_helper_one} provides a Jordan basis \(e_{ij}\) such that for all \(i \neq i_0\) and all \(j\), we have \(f(e_{ij}) = 0\).
    Restricting \(A\) to \(\langle e_{i_0j}\rangle_{j \textrm{ arbitrary}}\) gives a Jordan block, and then applying \cref{normalization_helper_two} gives the desired result.
\end{proof}

\subsection{Nilpotency Lemmas}
\begin{lemma}\label{upper_triangle_zero}
    Let \(X \in \gl_n\) be upper triangular.
    Then \(J_n + X\) is nilpotent if and only if \(X = 0\).
\end{lemma}
\begin{proof}
    Clearly if \(X = 0\), then \(J_n + X\) is nilpotent.
    Inversely, suppose \(X \neq 0\).
    Let \(e_1, ..., e_n\) be the standard basis, with \(J_n e_i = e_{i + 1}\).
    Let \(i_1 = \max\{i : Xe_i \neq 0\}\).
    As \(X\) is upper triangular, we have \(X e_{i_1} = v + ae_{i_2}\), with \(a \in \mathbb{C}\setminus\{0\}\), \(i_2 \leq i_1\), and \(v \in \langle e_1, ..., e_{i_1 - 1}\rangle\).
    \par Now, \((J_n + X)^{i_1} e_1 = e_{i_1 + 1} + v + ae_{i_2}\).
    Then, \((J_n + X)^{i_1 + (n - i_1)} e_1 = 0 + (J_n + X)^{n - i_1} (v + ae_{i_2})\).
    Clearly \((J_n + X)^{n - i_1} (v + ae_{i_2}) = v' + ae_{i_2 + n - i_1}\), with \(v' \in \langle e_1, ..., e_{i_2 + n - i_1 - 1} \rangle\).
    Now, since \(i_2 \leq i_1\), we have \(i_2 + n - i_1 \leq n\), and therefore \((J_n + X)^n e_1 \neq 0\).
    It follows that \(J_n + X\) is not nilpotent.
\end{proof}

\begin{lemma}\label{block_determinant}
    Let \(X \in \gl_m\), and let 
    \[Y = \begin{pmatrix}
        y_{11} & y_{12} & y_{13} & \cdots & y_{1,n-1} & y_{1n} \\
        d_1 & y_{22} & y_{23} & \cdots & y_{2,n-1} & y_{2n} \\
            & d_2   & y_{33} & \cdots & y_{3,n-1} & y_{3n} \\
            & & \ddots & \cdots & \vdots  & \vdots \\
            & & & d_{n - 2} & y_{n-1,n-1} & y_{n - 1,n}\\
            & & & & d_{n - 1} & y_{nn}
    \end{pmatrix} \in \gl_n.\]
    For any \(a, b \in \mathbb{C}^m\),
    \[\det 
    \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray} = \]
       \[\det X \det Y + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
            & & & \vert \\
            & X & & b     \\
            & & & \vert \\
            \hline 
        \text{---} & a & \text{---} & 0
       \end{pNiceArray}\]
\end{lemma}
\begin{proof}
    By induction on \(n\).
    In the case \(n = 1\), expanding along the last row (taking the usual interpretation of the empty product) gives the desired result.
    \par Now suppose \(n > 1\).
    Expanding along the last row, we get 
    \[d_{n - 1} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n-1} & \\
       & & &  \\
       \end{pNiceArray} -
    y_{nn} \det \begin{pNiceArray}{ccc|ccc}
        & & & & & \\
        & X & & & &   \\
        & & & & & \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y_{n,n} & \\
       & & &  \\
       \end{pNiceArray}.\]
    Using our inductive hypothesis for the first determiniant, and using that \(\det \begin{pNiceArray}{c|c}
        A_{11} & 0 \\
        A_{21} & A_{22}
    \end{pNiceArray} = \det A_{11} \det A_{22}\) for the second, the expression becomes
    \[d_{n - 1} \left(\det X \det Y_{n,n-1} + \left(\prod_{i \leq n - 2} d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}\right)
   - y_{nn} \det X \det Y_{nn} = \]
   \[(d_{n - 1} Y_{n,n-1} - y_{nn} \det Y_{nn})\det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray} = \]
   \[\det Y \det X + \left(\prod_i d_i\right) \det \begin{pNiceArray}{ccc|c}
        & & & \vert \\
        & X & & b     \\
        & & & \vert \\
        \hline 
    \text{---} & a & \text{---} & 0
   \end{pNiceArray}.\]
\end{proof}

\begin{corollary}\label{bottom_right_nilp}
    If \(X\) is nilpotent, and
    \[\begin{pNiceArray}{ccc|ccc}
        & & & & & \vert \\
        & X & & & & b    \\
        & & & & & \vert \\
       \hline
       \text{---} & a & \text{---} &  \\
       & & & & Y & \\
       & & &  \\
       \end{pNiceArray}\]
    is nilpotent  as well, then \(Y\) is nilpotent.
%     If all the \(d_i\) are nonzero, then
%     \[\begin{pNiceArray}{ccc|c}
%         & & & \vert \\
%         & X & & b     \\
%         & & & \vert \\
%         \hline 
%     \text{---} & a & \text{---} & 0
%    \end{pNiceArray}\]
%    is nilpotent as well.
% ^That is false.  I want to say something about its characteristic polynomial.  not sure that I care enough to take the space though.
    \begin{proof}
        By the previous lemma, the characteristic polynomial of the big matrix is
        \[g_X(\lambda) g_Y(\lambda) + f(\lambda),\]
        where \(g_X(\lambda) = \lambda^m\) is the characteristic polynomial of \(X\), and \(f(\lambda)\) is some polynomial of degree at most \(m - 1\).
    \end{proof}
\end{corollary}

\bibliography{paper}
\end{document}
